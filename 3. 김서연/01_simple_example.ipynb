{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  60000\n",
      "test :  10000\n"
     ]
    }
   ],
   "source": [
    "#MNIST Dataset (without standardization)\n",
    "\n",
    "train_data = dsets.MNIST(root = 'data/', train = True, transform = transforms.ToTensor(), download = True)\n",
    "test_data = dsets.MNIST(root = 'data/', train = False, transform = transforms.ToTensor(), download = True)\n",
    "\n",
    "print('train : ', len(train_data))\n",
    "print('test : ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Image ----------\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1176,\n",
      "        0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "        0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000])\n",
      "---------- Label ----------\n",
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = train_data[0]\n",
    "print('-'*10, 'Image', '-'*10)\n",
    "print(image[0][6])\n",
    "print('-'*10, 'Label', '-'*10)\n",
    "print(label)\n",
    "plt.imshow(image.squeeze().numpy(), cmap = 'gray')\n",
    "plt.title('%i' % label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  60000\n",
      "test :  10000\n"
     ]
    }
   ],
   "source": [
    "#MNIST Dataset (with standardization)\n",
    "standardizer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = (0.5), std = (0.5))\n",
    "])\n",
    "\n",
    "train_data = dsets.MNIST(root = 'data/', train = True, transform = standardizer, download = True)\n",
    "test_data = dsets.MNIST(root = 'data/', train = False, transform = standardizer, download = True)\n",
    "\n",
    "print('train : ', len(train_data))\n",
    "print('test : ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Image ----------\n",
      "tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -0.7647, -0.7176, -0.2627,  0.2078,  0.3333,  0.9843,  0.9843,  0.9843,\n",
      "         0.9843,  0.9843,  0.7647,  0.3490,  0.9843,  0.8980,  0.5294, -0.4980,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000])\n",
      "---------- Label ----------\n",
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = train_data[0]\n",
    "print('-'*10, 'Image', '-'*10)\n",
    "print(image[0][6])\n",
    "print('-'*10, 'Label', '-'*10)\n",
    "print(label)\n",
    "plt.imshow(image.squeeze().numpy(), cmap = 'gray')\n",
    "plt.title('%i' % label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + Dropout + Batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            #Conv block 1\n",
    "            nn.Conv2d(1, 32, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "            \n",
    "            #Conv Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout2d(p = 0.5),\n",
    "            nn.Linear(128*3*3, 625),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(625, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(x.size(0), -1)  #flatten\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "    \n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 819,749 trainable parameters. \n",
      "\n",
      "Classifier(\n",
      "  (conv_layer): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layer): Sequential(\n",
      "    (0): Dropout2d(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=1152, out_features=625, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=625, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(classifier):,} trainable parameters. \\n')\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True,\n",
    "                                         num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Start Training ...\n",
      "[1,     1] loss : 2.328\n",
      "[1,     2] loss : 2.333\n",
      "[1,     3] loss : 2.321\n",
      "[1,     4] loss : 2.327\n",
      "[1,     5] loss : 2.311\n",
      "[1,     6] loss : 2.308\n",
      "[1,     7] loss : 2.323\n",
      "[1,     8] loss : 2.313\n",
      "[1,     9] loss : 2.324\n",
      "[1,    10] loss : 2.325\n",
      "[1,    11] loss : 2.325\n",
      "[1,    12] loss : 2.323\n",
      "[1,    13] loss : 2.317\n",
      "[1,    14] loss : 2.314\n",
      "[1,    15] loss : 2.315\n",
      "[1,    16] loss : 2.311\n",
      "[1,    17] loss : 2.305\n",
      "[1,    18] loss : 2.310\n",
      "[1,    19] loss : 2.321\n",
      "[1,    20] loss : 2.311\n",
      "[1,    21] loss : 2.304\n",
      "[1,    22] loss : 2.305\n",
      "[1,    23] loss : 2.302\n",
      "[1,    24] loss : 2.303\n",
      "[1,    25] loss : 2.308\n",
      "[1,    26] loss : 2.299\n",
      "[1,    27] loss : 2.296\n",
      "[1,    28] loss : 2.302\n",
      "[1,    29] loss : 2.312\n",
      "[1,    30] loss : 2.309\n",
      "[1,    31] loss : 2.308\n",
      "[1,    32] loss : 2.305\n",
      "[1,    33] loss : 2.302\n",
      "[1,    34] loss : 2.301\n",
      "[1,    35] loss : 2.304\n",
      "[1,    36] loss : 2.293\n",
      "[1,    37] loss : 2.307\n",
      "[1,    38] loss : 2.293\n",
      "[1,    39] loss : 2.299\n",
      "[1,    40] loss : 2.298\n",
      "[1,    41] loss : 2.305\n",
      "[1,    42] loss : 2.308\n",
      "[1,    43] loss : 2.298\n",
      "[1,    44] loss : 2.302\n",
      "[1,    45] loss : 2.292\n",
      "[1,    46] loss : 2.298\n",
      "[1,    47] loss : 2.301\n",
      "[1,    48] loss : 2.307\n",
      "[1,    49] loss : 2.287\n",
      "[1,    50] loss : 2.295\n",
      "[1,    51] loss : 2.293\n",
      "[1,    52] loss : 2.286\n",
      "[1,    53] loss : 2.288\n",
      "[1,    54] loss : 2.298\n",
      "[1,    55] loss : 2.288\n",
      "[1,    56] loss : 2.293\n",
      "[1,    57] loss : 2.284\n",
      "[1,    58] loss : 2.288\n",
      "[1,    59] loss : 2.291\n",
      "[1,    60] loss : 2.293\n",
      "[2,     1] loss : 2.286\n",
      "[2,     2] loss : 2.297\n",
      "[2,     3] loss : 2.285\n",
      "[2,     4] loss : 2.285\n",
      "[2,     5] loss : 2.282\n",
      "[2,     6] loss : 2.281\n",
      "[2,     7] loss : 2.282\n",
      "[2,     8] loss : 2.290\n",
      "[2,     9] loss : 2.278\n",
      "[2,    10] loss : 2.287\n",
      "[2,    11] loss : 2.289\n",
      "[2,    12] loss : 2.282\n",
      "[2,    13] loss : 2.279\n",
      "[2,    14] loss : 2.279\n",
      "[2,    15] loss : 2.280\n",
      "[2,    16] loss : 2.282\n",
      "[2,    17] loss : 2.283\n",
      "[2,    18] loss : 2.276\n",
      "[2,    19] loss : 2.285\n",
      "[2,    20] loss : 2.278\n",
      "[2,    21] loss : 2.282\n",
      "[2,    22] loss : 2.277\n",
      "[2,    23] loss : 2.278\n",
      "[2,    24] loss : 2.268\n",
      "[2,    25] loss : 2.274\n",
      "[2,    26] loss : 2.278\n",
      "[2,    27] loss : 2.281\n",
      "[2,    28] loss : 2.265\n",
      "[2,    29] loss : 2.273\n",
      "[2,    30] loss : 2.284\n",
      "[2,    31] loss : 2.263\n",
      "[2,    32] loss : 2.265\n",
      "[2,    33] loss : 2.271\n",
      "[2,    34] loss : 2.270\n",
      "[2,    35] loss : 2.267\n",
      "[2,    36] loss : 2.266\n",
      "[2,    37] loss : 2.264\n",
      "[2,    38] loss : 2.274\n",
      "[2,    39] loss : 2.264\n",
      "[2,    40] loss : 2.266\n",
      "[2,    41] loss : 2.268\n",
      "[2,    42] loss : 2.265\n",
      "[2,    43] loss : 2.274\n",
      "[2,    44] loss : 2.270\n",
      "[2,    45] loss : 2.268\n",
      "[2,    46] loss : 2.269\n",
      "[2,    47] loss : 2.265\n",
      "[2,    48] loss : 2.260\n",
      "[2,    49] loss : 2.264\n",
      "[2,    50] loss : 2.271\n",
      "[2,    51] loss : 2.259\n",
      "[2,    52] loss : 2.268\n",
      "[2,    53] loss : 2.259\n",
      "[2,    54] loss : 2.261\n",
      "[2,    55] loss : 2.269\n",
      "[2,    56] loss : 2.264\n",
      "[2,    57] loss : 2.258\n",
      "[2,    58] loss : 2.255\n",
      "[2,    59] loss : 2.267\n",
      "[2,    60] loss : 2.265\n",
      "[3,     1] loss : 2.252\n",
      "[3,     2] loss : 2.266\n",
      "[3,     3] loss : 2.266\n",
      "[3,     4] loss : 2.249\n",
      "[3,     5] loss : 2.253\n",
      "[3,     6] loss : 2.256\n",
      "[3,     7] loss : 2.258\n",
      "[3,     8] loss : 2.247\n",
      "[3,     9] loss : 2.250\n",
      "[3,    10] loss : 2.254\n",
      "[3,    11] loss : 2.246\n",
      "[3,    12] loss : 2.259\n",
      "[3,    13] loss : 2.248\n",
      "[3,    14] loss : 2.246\n",
      "[3,    15] loss : 2.255\n",
      "[3,    16] loss : 2.251\n",
      "[3,    17] loss : 2.250\n",
      "[3,    18] loss : 2.245\n",
      "[3,    19] loss : 2.251\n",
      "[3,    20] loss : 2.252\n",
      "[3,    21] loss : 2.247\n",
      "[3,    22] loss : 2.248\n",
      "[3,    23] loss : 2.249\n",
      "[3,    24] loss : 2.246\n",
      "[3,    25] loss : 2.250\n",
      "[3,    26] loss : 2.249\n",
      "[3,    27] loss : 2.240\n",
      "[3,    28] loss : 2.248\n",
      "[3,    29] loss : 2.240\n",
      "[3,    30] loss : 2.247\n",
      "[3,    31] loss : 2.236\n",
      "[3,    32] loss : 2.251\n",
      "[3,    33] loss : 2.248\n",
      "[3,    34] loss : 2.243\n",
      "[3,    35] loss : 2.245\n",
      "[3,    36] loss : 2.242\n",
      "[3,    37] loss : 2.245\n",
      "[3,    38] loss : 2.238\n",
      "[3,    39] loss : 2.239\n",
      "[3,    40] loss : 2.241\n",
      "[3,    41] loss : 2.237\n",
      "[3,    42] loss : 2.236\n",
      "[3,    43] loss : 2.239\n",
      "[3,    44] loss : 2.243\n",
      "[3,    45] loss : 2.245\n",
      "[3,    46] loss : 2.234\n",
      "[3,    47] loss : 2.241\n",
      "[3,    48] loss : 2.240\n",
      "[3,    49] loss : 2.240\n",
      "[3,    50] loss : 2.234\n",
      "[3,    51] loss : 2.251\n",
      "[3,    52] loss : 2.227\n",
      "[3,    53] loss : 2.234\n",
      "[3,    54] loss : 2.232\n",
      "[3,    55] loss : 2.230\n",
      "[3,    56] loss : 2.234\n",
      "[3,    57] loss : 2.232\n",
      "[3,    58] loss : 2.236\n",
      "[3,    59] loss : 2.228\n",
      "[3,    60] loss : 2.229\n",
      "[4,     1] loss : 2.230\n",
      "[4,     2] loss : 2.226\n",
      "[4,     3] loss : 2.237\n",
      "[4,     4] loss : 2.228\n",
      "[4,     5] loss : 2.228\n",
      "[4,     6] loss : 2.224\n",
      "[4,     7] loss : 2.226\n",
      "[4,     8] loss : 2.226\n",
      "[4,     9] loss : 2.217\n",
      "[4,    10] loss : 2.221\n",
      "[4,    11] loss : 2.220\n",
      "[4,    12] loss : 2.227\n",
      "[4,    13] loss : 2.219\n",
      "[4,    14] loss : 2.224\n",
      "[4,    15] loss : 2.223\n",
      "[4,    16] loss : 2.226\n",
      "[4,    17] loss : 2.222\n",
      "[4,    18] loss : 2.220\n",
      "[4,    19] loss : 2.210\n",
      "[4,    20] loss : 2.218\n",
      "[4,    21] loss : 2.214\n",
      "[4,    22] loss : 2.220\n",
      "[4,    23] loss : 2.227\n",
      "[4,    24] loss : 2.218\n",
      "[4,    25] loss : 2.211\n",
      "[4,    26] loss : 2.218\n",
      "[4,    27] loss : 2.214\n",
      "[4,    28] loss : 2.217\n",
      "[4,    29] loss : 2.216\n",
      "[4,    30] loss : 2.213\n",
      "[4,    31] loss : 2.221\n",
      "[4,    32] loss : 2.207\n",
      "[4,    33] loss : 2.214\n",
      "[4,    34] loss : 2.212\n",
      "[4,    35] loss : 2.219\n",
      "[4,    36] loss : 2.209\n",
      "[4,    37] loss : 2.216\n",
      "[4,    38] loss : 2.206\n",
      "[4,    39] loss : 2.207\n",
      "[4,    40] loss : 2.211\n",
      "[4,    41] loss : 2.206\n",
      "[4,    42] loss : 2.214\n",
      "[4,    43] loss : 2.214\n",
      "[4,    44] loss : 2.207\n",
      "[4,    45] loss : 2.214\n",
      "[4,    46] loss : 2.205\n",
      "[4,    47] loss : 2.208\n",
      "[4,    48] loss : 2.210\n",
      "[4,    49] loss : 2.203\n",
      "[4,    50] loss : 2.212\n",
      "[4,    51] loss : 2.201\n",
      "[4,    52] loss : 2.201\n",
      "[4,    53] loss : 2.208\n",
      "[4,    54] loss : 2.197\n",
      "[4,    55] loss : 2.203\n",
      "[4,    56] loss : 2.205\n",
      "[4,    57] loss : 2.198\n",
      "[4,    58] loss : 2.197\n",
      "[4,    59] loss : 2.198\n",
      "[4,    60] loss : 2.195\n",
      "[5,     1] loss : 2.200\n",
      "[5,     2] loss : 2.200\n",
      "[5,     3] loss : 2.196\n",
      "[5,     4] loss : 2.202\n",
      "[5,     5] loss : 2.196\n",
      "[5,     6] loss : 2.194\n",
      "[5,     7] loss : 2.204\n",
      "[5,     8] loss : 2.199\n",
      "[5,     9] loss : 2.200\n",
      "[5,    10] loss : 2.205\n",
      "[5,    11] loss : 2.202\n",
      "[5,    12] loss : 2.190\n",
      "[5,    13] loss : 2.193\n",
      "[5,    14] loss : 2.190\n",
      "[5,    15] loss : 2.197\n",
      "[5,    16] loss : 2.188\n",
      "[5,    17] loss : 2.193\n",
      "[5,    18] loss : 2.184\n",
      "[5,    19] loss : 2.188\n",
      "[5,    20] loss : 2.191\n",
      "[5,    21] loss : 2.187\n",
      "[5,    22] loss : 2.193\n",
      "[5,    23] loss : 2.191\n",
      "[5,    24] loss : 2.185\n",
      "[5,    25] loss : 2.183\n",
      "[5,    26] loss : 2.192\n",
      "[5,    27] loss : 2.190\n",
      "[5,    28] loss : 2.184\n",
      "[5,    29] loss : 2.189\n",
      "[5,    30] loss : 2.193\n",
      "[5,    31] loss : 2.185\n",
      "[5,    32] loss : 2.187\n",
      "[5,    33] loss : 2.173\n",
      "[5,    34] loss : 2.185\n",
      "[5,    35] loss : 2.185\n",
      "[5,    36] loss : 2.190\n",
      "[5,    37] loss : 2.182\n",
      "[5,    38] loss : 2.183\n",
      "[5,    39] loss : 2.168\n",
      "[5,    40] loss : 2.172\n",
      "[5,    41] loss : 2.181\n",
      "[5,    42] loss : 2.183\n",
      "[5,    43] loss : 2.182\n",
      "[5,    44] loss : 2.181\n",
      "[5,    45] loss : 2.183\n",
      "[5,    46] loss : 2.175\n",
      "[5,    47] loss : 2.172\n",
      "[5,    48] loss : 2.180\n",
      "[5,    49] loss : 2.173\n",
      "[5,    50] loss : 2.178\n",
      "[5,    51] loss : 2.176\n",
      "[5,    52] loss : 2.169\n",
      "[5,    53] loss : 2.183\n",
      "[5,    54] loss : 2.174\n",
      "[5,    55] loss : 2.171\n",
      "[5,    56] loss : 2.179\n",
      "[5,    57] loss : 2.174\n",
      "[5,    58] loss : 2.162\n",
      "[5,    59] loss : 2.172\n",
      "[5,    60] loss : 2.172\n",
      "[6,     1] loss : 2.164\n",
      "[6,     2] loss : 2.168\n",
      "[6,     3] loss : 2.167\n",
      "[6,     4] loss : 2.173\n",
      "[6,     5] loss : 2.166\n",
      "[6,     6] loss : 2.161\n",
      "[6,     7] loss : 2.161\n",
      "[6,     8] loss : 2.173\n",
      "[6,     9] loss : 2.163\n",
      "[6,    10] loss : 2.159\n",
      "[6,    11] loss : 2.163\n",
      "[6,    12] loss : 2.162\n",
      "[6,    13] loss : 2.160\n",
      "[6,    14] loss : 2.151\n",
      "[6,    15] loss : 2.164\n",
      "[6,    16] loss : 2.151\n",
      "[6,    17] loss : 2.156\n",
      "[6,    18] loss : 2.152\n",
      "[6,    19] loss : 2.155\n",
      "[6,    20] loss : 2.151\n",
      "[6,    21] loss : 2.158\n",
      "[6,    22] loss : 2.155\n",
      "[6,    23] loss : 2.152\n",
      "[6,    24] loss : 2.150\n",
      "[6,    25] loss : 2.156\n",
      "[6,    26] loss : 2.150\n",
      "[6,    27] loss : 2.160\n",
      "[6,    28] loss : 2.156\n",
      "[6,    29] loss : 2.151\n",
      "[6,    30] loss : 2.159\n",
      "[6,    31] loss : 2.156\n",
      "[6,    32] loss : 2.155\n",
      "[6,    33] loss : 2.145\n",
      "[6,    34] loss : 2.146\n",
      "[6,    35] loss : 2.138\n",
      "[6,    36] loss : 2.144\n",
      "[6,    37] loss : 2.152\n",
      "[6,    38] loss : 2.147\n",
      "[6,    39] loss : 2.152\n",
      "[6,    40] loss : 2.142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,    41] loss : 2.141\n",
      "[6,    42] loss : 2.153\n",
      "[6,    43] loss : 2.144\n",
      "[6,    44] loss : 2.146\n",
      "[6,    45] loss : 2.135\n",
      "[6,    46] loss : 2.141\n",
      "[6,    47] loss : 2.140\n",
      "[6,    48] loss : 2.138\n",
      "[6,    49] loss : 2.136\n",
      "[6,    50] loss : 2.132\n",
      "[6,    51] loss : 2.142\n",
      "[6,    52] loss : 2.149\n",
      "[6,    53] loss : 2.132\n",
      "[6,    54] loss : 2.133\n",
      "[6,    55] loss : 2.134\n",
      "[6,    56] loss : 2.138\n",
      "[6,    57] loss : 2.132\n",
      "[6,    58] loss : 2.136\n",
      "[6,    59] loss : 2.135\n",
      "[6,    60] loss : 2.142\n",
      "[7,     1] loss : 2.134\n",
      "[7,     2] loss : 2.138\n",
      "[7,     3] loss : 2.126\n",
      "[7,     4] loss : 2.136\n",
      "[7,     5] loss : 2.135\n",
      "[7,     6] loss : 2.128\n",
      "[7,     7] loss : 2.134\n",
      "[7,     8] loss : 2.128\n",
      "[7,     9] loss : 2.138\n",
      "[7,    10] loss : 2.125\n",
      "[7,    11] loss : 2.136\n",
      "[7,    12] loss : 2.129\n",
      "[7,    13] loss : 2.131\n",
      "[7,    14] loss : 2.123\n",
      "[7,    15] loss : 2.126\n",
      "[7,    16] loss : 2.118\n",
      "[7,    17] loss : 2.122\n",
      "[7,    18] loss : 2.119\n",
      "[7,    19] loss : 2.127\n",
      "[7,    20] loss : 2.123\n",
      "[7,    21] loss : 2.113\n",
      "[7,    22] loss : 2.115\n",
      "[7,    23] loss : 2.119\n",
      "[7,    24] loss : 2.131\n",
      "[7,    25] loss : 2.131\n",
      "[7,    26] loss : 2.124\n",
      "[7,    27] loss : 2.106\n",
      "[7,    28] loss : 2.114\n",
      "[7,    29] loss : 2.106\n",
      "[7,    30] loss : 2.122\n",
      "[7,    31] loss : 2.113\n",
      "[7,    32] loss : 2.105\n",
      "[7,    33] loss : 2.109\n",
      "[7,    34] loss : 2.114\n",
      "[7,    35] loss : 2.109\n",
      "[7,    36] loss : 2.120\n",
      "[7,    37] loss : 2.111\n",
      "[7,    38] loss : 2.116\n",
      "[7,    39] loss : 2.116\n",
      "[7,    40] loss : 2.101\n",
      "[7,    41] loss : 2.099\n",
      "[7,    42] loss : 2.100\n",
      "[7,    43] loss : 2.104\n",
      "[7,    44] loss : 2.108\n",
      "[7,    45] loss : 2.103\n",
      "[7,    46] loss : 2.113\n",
      "[7,    47] loss : 2.106\n",
      "[7,    48] loss : 2.107\n",
      "[7,    49] loss : 2.106\n",
      "[7,    50] loss : 2.111\n",
      "[7,    51] loss : 2.111\n",
      "[7,    52] loss : 2.105\n",
      "[7,    53] loss : 2.093\n",
      "[7,    54] loss : 2.090\n",
      "[7,    55] loss : 2.107\n",
      "[7,    56] loss : 2.092\n",
      "[7,    57] loss : 2.088\n",
      "[7,    58] loss : 2.096\n",
      "[7,    59] loss : 2.091\n",
      "[7,    60] loss : 2.091\n",
      "[8,     1] loss : 2.093\n",
      "[8,     2] loss : 2.095\n",
      "[8,     3] loss : 2.092\n",
      "[8,     4] loss : 2.089\n",
      "[8,     5] loss : 2.084\n",
      "[8,     6] loss : 2.089\n",
      "[8,     7] loss : 2.095\n",
      "[8,     8] loss : 2.091\n",
      "[8,     9] loss : 2.079\n",
      "[8,    10] loss : 2.080\n",
      "[8,    11] loss : 2.075\n",
      "[8,    12] loss : 2.070\n",
      "[8,    13] loss : 2.085\n",
      "[8,    14] loss : 2.080\n",
      "[8,    15] loss : 2.076\n",
      "[8,    16] loss : 2.087\n",
      "[8,    17] loss : 2.079\n",
      "[8,    18] loss : 2.087\n",
      "[8,    19] loss : 2.083\n",
      "[8,    20] loss : 2.076\n",
      "[8,    21] loss : 2.086\n",
      "[8,    22] loss : 2.070\n",
      "[8,    23] loss : 2.087\n",
      "[8,    24] loss : 2.070\n",
      "[8,    25] loss : 2.080\n",
      "[8,    26] loss : 2.077\n",
      "[8,    27] loss : 2.070\n",
      "[8,    28] loss : 2.074\n",
      "[8,    29] loss : 2.077\n",
      "[8,    30] loss : 2.063\n",
      "[8,    31] loss : 2.072\n",
      "[8,    32] loss : 2.086\n",
      "[8,    33] loss : 2.062\n",
      "[8,    34] loss : 2.070\n",
      "[8,    35] loss : 2.070\n",
      "[8,    36] loss : 2.067\n",
      "[8,    37] loss : 2.056\n",
      "[8,    38] loss : 2.060\n",
      "[8,    39] loss : 2.079\n",
      "[8,    40] loss : 2.072\n",
      "[8,    41] loss : 2.062\n",
      "[8,    42] loss : 2.060\n",
      "[8,    43] loss : 2.068\n",
      "[8,    44] loss : 2.064\n",
      "[8,    45] loss : 2.066\n",
      "[8,    46] loss : 2.058\n",
      "[8,    47] loss : 2.058\n",
      "[8,    48] loss : 2.058\n",
      "[8,    49] loss : 2.048\n",
      "[8,    50] loss : 2.060\n",
      "[8,    51] loss : 2.053\n",
      "[8,    52] loss : 2.049\n",
      "[8,    53] loss : 2.044\n",
      "[8,    54] loss : 2.054\n",
      "[8,    55] loss : 2.052\n",
      "[8,    56] loss : 2.055\n",
      "[8,    57] loss : 2.039\n",
      "[8,    58] loss : 2.032\n",
      "[8,    59] loss : 2.053\n",
      "[8,    60] loss : 2.057\n",
      "[9,     1] loss : 2.041\n",
      "[9,     2] loss : 2.050\n",
      "[9,     3] loss : 2.039\n",
      "[9,     4] loss : 2.040\n",
      "[9,     5] loss : 2.041\n",
      "[9,     6] loss : 2.035\n",
      "[9,     7] loss : 2.044\n",
      "[9,     8] loss : 2.037\n",
      "[9,     9] loss : 2.039\n",
      "[9,    10] loss : 2.032\n",
      "[9,    11] loss : 2.040\n",
      "[9,    12] loss : 2.035\n",
      "[9,    13] loss : 2.025\n",
      "[9,    14] loss : 2.034\n",
      "[9,    15] loss : 2.049\n",
      "[9,    16] loss : 2.038\n",
      "[9,    17] loss : 2.034\n",
      "[9,    18] loss : 2.031\n",
      "[9,    19] loss : 2.037\n",
      "[9,    20] loss : 2.027\n",
      "[9,    21] loss : 2.028\n",
      "[9,    22] loss : 2.027\n",
      "[9,    23] loss : 2.012\n",
      "[9,    24] loss : 2.030\n",
      "[9,    25] loss : 2.029\n",
      "[9,    26] loss : 2.022\n",
      "[9,    27] loss : 2.022\n",
      "[9,    28] loss : 2.021\n",
      "[9,    29] loss : 2.012\n",
      "[9,    30] loss : 2.010\n",
      "[9,    31] loss : 2.021\n",
      "[9,    32] loss : 2.026\n",
      "[9,    33] loss : 2.021\n",
      "[9,    34] loss : 2.012\n",
      "[9,    35] loss : 2.016\n",
      "[9,    36] loss : 2.018\n",
      "[9,    37] loss : 2.029\n",
      "[9,    38] loss : 2.010\n",
      "[9,    39] loss : 2.012\n",
      "[9,    40] loss : 2.002\n",
      "[9,    41] loss : 2.016\n",
      "[9,    42] loss : 2.011\n",
      "[9,    43] loss : 2.011\n",
      "[9,    44] loss : 2.024\n",
      "[9,    45] loss : 2.027\n",
      "[9,    46] loss : 2.021\n",
      "[9,    47] loss : 2.018\n",
      "[9,    48] loss : 2.007\n",
      "[9,    49] loss : 2.010\n",
      "[9,    50] loss : 2.008\n",
      "[9,    51] loss : 2.008\n",
      "[9,    52] loss : 2.002\n",
      "[9,    53] loss : 1.996\n",
      "[9,    54] loss : 1.992\n",
      "[9,    55] loss : 1.996\n",
      "[9,    56] loss : 2.006\n",
      "[9,    57] loss : 1.996\n",
      "[9,    58] loss : 2.004\n",
      "[9,    59] loss : 2.010\n",
      "[9,    60] loss : 1.990\n",
      "[10,     1] loss : 1.993\n",
      "[10,     2] loss : 1.987\n",
      "[10,     3] loss : 1.977\n",
      "[10,     4] loss : 1.990\n",
      "[10,     5] loss : 1.982\n",
      "[10,     6] loss : 2.003\n",
      "[10,     7] loss : 1.992\n",
      "[10,     8] loss : 2.001\n",
      "[10,     9] loss : 1.978\n",
      "[10,    10] loss : 1.986\n",
      "[10,    11] loss : 1.984\n",
      "[10,    12] loss : 1.980\n",
      "[10,    13] loss : 1.980\n",
      "[10,    14] loss : 1.986\n",
      "[10,    15] loss : 1.982\n",
      "[10,    16] loss : 1.981\n",
      "[10,    17] loss : 1.975\n",
      "[10,    18] loss : 1.983\n",
      "[10,    19] loss : 1.968\n",
      "[10,    20] loss : 1.972\n",
      "[10,    21] loss : 1.979\n",
      "[10,    22] loss : 1.964\n",
      "[10,    23] loss : 1.967\n",
      "[10,    24] loss : 1.969\n",
      "[10,    25] loss : 1.978\n",
      "[10,    26] loss : 1.971\n",
      "[10,    27] loss : 1.968\n",
      "[10,    28] loss : 1.967\n",
      "[10,    29] loss : 1.978\n",
      "[10,    30] loss : 1.969\n",
      "[10,    31] loss : 1.973\n",
      "[10,    32] loss : 1.954\n",
      "[10,    33] loss : 1.962\n",
      "[10,    34] loss : 1.963\n",
      "[10,    35] loss : 1.944\n",
      "[10,    36] loss : 1.969\n",
      "[10,    37] loss : 1.965\n",
      "[10,    38] loss : 1.955\n",
      "[10,    39] loss : 1.957\n",
      "[10,    40] loss : 1.963\n",
      "[10,    41] loss : 1.952\n",
      "[10,    42] loss : 1.955\n",
      "[10,    43] loss : 1.948\n",
      "[10,    44] loss : 1.941\n",
      "[10,    45] loss : 1.956\n",
      "[10,    46] loss : 1.966\n",
      "[10,    47] loss : 1.950\n",
      "[10,    48] loss : 1.948\n",
      "[10,    49] loss : 1.947\n",
      "[10,    50] loss : 1.944\n",
      "[10,    51] loss : 1.954\n",
      "[10,    52] loss : 1.937\n",
      "[10,    53] loss : 1.947\n",
      "[10,    54] loss : 1.948\n",
      "[10,    55] loss : 1.941\n",
      "[10,    56] loss : 1.948\n",
      "[10,    57] loss : 1.944\n",
      "[10,    58] loss : 1.950\n",
      "[10,    59] loss : 1.938\n",
      "[10,    60] loss : 1.933\n",
      "[11,     1] loss : 1.925\n",
      "[11,     2] loss : 1.925\n",
      "[11,     3] loss : 1.923\n",
      "[11,     4] loss : 1.939\n",
      "[11,     5] loss : 1.931\n",
      "[11,     6] loss : 1.936\n",
      "[11,     7] loss : 1.929\n",
      "[11,     8] loss : 1.925\n",
      "[11,     9] loss : 1.917\n",
      "[11,    10] loss : 1.930\n",
      "[11,    11] loss : 1.939\n",
      "[11,    12] loss : 1.911\n",
      "[11,    13] loss : 1.919\n",
      "[11,    14] loss : 1.930\n",
      "[11,    15] loss : 1.927\n",
      "[11,    16] loss : 1.930\n",
      "[11,    17] loss : 1.903\n",
      "[11,    18] loss : 1.912\n",
      "[11,    19] loss : 1.916\n",
      "[11,    20] loss : 1.916\n",
      "[11,    21] loss : 1.909\n",
      "[11,    22] loss : 1.903\n",
      "[11,    23] loss : 1.906\n",
      "[11,    24] loss : 1.901\n",
      "[11,    25] loss : 1.909\n",
      "[11,    26] loss : 1.891\n",
      "[11,    27] loss : 1.915\n",
      "[11,    28] loss : 1.920\n",
      "[11,    29] loss : 1.924\n",
      "[11,    30] loss : 1.900\n",
      "[11,    31] loss : 1.906\n",
      "[11,    32] loss : 1.890\n",
      "[11,    33] loss : 1.895\n",
      "[11,    34] loss : 1.897\n",
      "[11,    35] loss : 1.891\n",
      "[11,    36] loss : 1.895\n",
      "[11,    37] loss : 1.907\n",
      "[11,    38] loss : 1.900\n",
      "[11,    39] loss : 1.892\n",
      "[11,    40] loss : 1.900\n",
      "[11,    41] loss : 1.886\n",
      "[11,    42] loss : 1.907\n",
      "[11,    43] loss : 1.873\n",
      "[11,    44] loss : 1.884\n",
      "[11,    45] loss : 1.897\n",
      "[11,    46] loss : 1.876\n",
      "[11,    47] loss : 1.880\n",
      "[11,    48] loss : 1.875\n",
      "[11,    49] loss : 1.883\n",
      "[11,    50] loss : 1.881\n",
      "[11,    51] loss : 1.883\n",
      "[11,    52] loss : 1.878\n",
      "[11,    53] loss : 1.872\n",
      "[11,    54] loss : 1.878\n",
      "[11,    55] loss : 1.876\n",
      "[11,    56] loss : 1.866\n",
      "[11,    57] loss : 1.879\n",
      "[11,    58] loss : 1.879\n",
      "[11,    59] loss : 1.873\n",
      "[11,    60] loss : 1.851\n",
      "[12,     1] loss : 1.853\n",
      "[12,     2] loss : 1.872\n",
      "[12,     3] loss : 1.850\n",
      "[12,     4] loss : 1.863\n",
      "[12,     5] loss : 1.855\n",
      "[12,     6] loss : 1.875\n",
      "[12,     7] loss : 1.866\n",
      "[12,     8] loss : 1.872\n",
      "[12,     9] loss : 1.857\n",
      "[12,    10] loss : 1.867\n",
      "[12,    11] loss : 1.845\n",
      "[12,    12] loss : 1.847\n",
      "[12,    13] loss : 1.847\n",
      "[12,    14] loss : 1.860\n",
      "[12,    15] loss : 1.851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12,    16] loss : 1.859\n",
      "[12,    17] loss : 1.851\n",
      "[12,    18] loss : 1.853\n",
      "[12,    19] loss : 1.849\n",
      "[12,    20] loss : 1.851\n",
      "[12,    21] loss : 1.845\n",
      "[12,    22] loss : 1.813\n",
      "[12,    23] loss : 1.846\n",
      "[12,    24] loss : 1.844\n",
      "[12,    25] loss : 1.838\n",
      "[12,    26] loss : 1.836\n",
      "[12,    27] loss : 1.823\n",
      "[12,    28] loss : 1.827\n",
      "[12,    29] loss : 1.828\n",
      "[12,    30] loss : 1.836\n",
      "[12,    31] loss : 1.822\n",
      "[12,    32] loss : 1.823\n",
      "[12,    33] loss : 1.839\n",
      "[12,    34] loss : 1.832\n",
      "[12,    35] loss : 1.833\n",
      "[12,    36] loss : 1.814\n",
      "[12,    37] loss : 1.825\n",
      "[12,    38] loss : 1.825\n",
      "[12,    39] loss : 1.828\n",
      "[12,    40] loss : 1.814\n",
      "[12,    41] loss : 1.803\n",
      "[12,    42] loss : 1.817\n",
      "[12,    43] loss : 1.810\n",
      "[12,    44] loss : 1.815\n",
      "[12,    45] loss : 1.828\n",
      "[12,    46] loss : 1.792\n",
      "[12,    47] loss : 1.812\n",
      "[12,    48] loss : 1.812\n",
      "[12,    49] loss : 1.818\n",
      "[12,    50] loss : 1.800\n",
      "[12,    51] loss : 1.812\n",
      "[12,    52] loss : 1.796\n",
      "[12,    53] loss : 1.790\n",
      "[12,    54] loss : 1.800\n",
      "[12,    55] loss : 1.805\n",
      "[12,    56] loss : 1.789\n",
      "[12,    57] loss : 1.798\n",
      "[12,    58] loss : 1.798\n",
      "[12,    59] loss : 1.787\n",
      "[12,    60] loss : 1.801\n",
      "[13,     1] loss : 1.786\n",
      "[13,     2] loss : 1.792\n",
      "[13,     3] loss : 1.767\n",
      "[13,     4] loss : 1.785\n",
      "[13,     5] loss : 1.791\n",
      "[13,     6] loss : 1.789\n",
      "[13,     7] loss : 1.784\n",
      "[13,     8] loss : 1.784\n",
      "[13,     9] loss : 1.771\n",
      "[13,    10] loss : 1.792\n",
      "[13,    11] loss : 1.781\n",
      "[13,    12] loss : 1.797\n",
      "[13,    13] loss : 1.779\n",
      "[13,    14] loss : 1.784\n",
      "[13,    15] loss : 1.765\n",
      "[13,    16] loss : 1.778\n",
      "[13,    17] loss : 1.778\n",
      "[13,    18] loss : 1.782\n",
      "[13,    19] loss : 1.774\n",
      "[13,    20] loss : 1.743\n",
      "[13,    21] loss : 1.759\n",
      "[13,    22] loss : 1.762\n",
      "[13,    23] loss : 1.770\n",
      "[13,    24] loss : 1.779\n",
      "[13,    25] loss : 1.766\n",
      "[13,    26] loss : 1.759\n",
      "[13,    27] loss : 1.758\n",
      "[13,    28] loss : 1.750\n",
      "[13,    29] loss : 1.750\n",
      "[13,    30] loss : 1.742\n",
      "[13,    31] loss : 1.755\n",
      "[13,    32] loss : 1.771\n",
      "[13,    33] loss : 1.758\n",
      "[13,    34] loss : 1.737\n",
      "[13,    35] loss : 1.741\n",
      "[13,    36] loss : 1.746\n",
      "[13,    37] loss : 1.765\n",
      "[13,    38] loss : 1.742\n",
      "[13,    39] loss : 1.736\n",
      "[13,    40] loss : 1.761\n",
      "[13,    41] loss : 1.714\n",
      "[13,    42] loss : 1.727\n",
      "[13,    43] loss : 1.720\n",
      "[13,    44] loss : 1.703\n",
      "[13,    45] loss : 1.727\n",
      "[13,    46] loss : 1.723\n",
      "[13,    47] loss : 1.739\n",
      "[13,    48] loss : 1.722\n",
      "[13,    49] loss : 1.720\n",
      "[13,    50] loss : 1.718\n",
      "[13,    51] loss : 1.736\n",
      "[13,    52] loss : 1.713\n",
      "[13,    53] loss : 1.696\n",
      "[13,    54] loss : 1.733\n",
      "[13,    55] loss : 1.733\n",
      "[13,    56] loss : 1.740\n",
      "[13,    57] loss : 1.679\n",
      "[13,    58] loss : 1.714\n",
      "[13,    59] loss : 1.705\n",
      "[13,    60] loss : 1.690\n",
      "[14,     1] loss : 1.690\n",
      "[14,     2] loss : 1.705\n",
      "[14,     3] loss : 1.711\n",
      "[14,     4] loss : 1.701\n",
      "[14,     5] loss : 1.688\n",
      "[14,     6] loss : 1.702\n",
      "[14,     7] loss : 1.694\n",
      "[14,     8] loss : 1.695\n",
      "[14,     9] loss : 1.690\n",
      "[14,    10] loss : 1.678\n",
      "[14,    11] loss : 1.677\n",
      "[14,    12] loss : 1.677\n",
      "[14,    13] loss : 1.691\n",
      "[14,    14] loss : 1.692\n",
      "[14,    15] loss : 1.697\n",
      "[14,    16] loss : 1.669\n",
      "[14,    17] loss : 1.672\n",
      "[14,    18] loss : 1.674\n",
      "[14,    19] loss : 1.669\n",
      "[14,    20] loss : 1.686\n",
      "[14,    21] loss : 1.685\n",
      "[14,    22] loss : 1.689\n",
      "[14,    23] loss : 1.681\n",
      "[14,    24] loss : 1.656\n",
      "[14,    25] loss : 1.692\n",
      "[14,    26] loss : 1.650\n",
      "[14,    27] loss : 1.665\n",
      "[14,    28] loss : 1.667\n",
      "[14,    29] loss : 1.661\n",
      "[14,    30] loss : 1.648\n",
      "[14,    31] loss : 1.668\n",
      "[14,    32] loss : 1.648\n",
      "[14,    33] loss : 1.674\n",
      "[14,    34] loss : 1.646\n",
      "[14,    35] loss : 1.658\n",
      "[14,    36] loss : 1.655\n",
      "[14,    37] loss : 1.651\n",
      "[14,    38] loss : 1.670\n",
      "[14,    39] loss : 1.673\n",
      "[14,    40] loss : 1.641\n",
      "[14,    41] loss : 1.654\n",
      "[14,    42] loss : 1.662\n",
      "[14,    43] loss : 1.676\n",
      "[14,    44] loss : 1.639\n",
      "[14,    45] loss : 1.641\n",
      "[14,    46] loss : 1.639\n",
      "[14,    47] loss : 1.643\n",
      "[14,    48] loss : 1.632\n",
      "[14,    49] loss : 1.638\n",
      "[14,    50] loss : 1.660\n",
      "[14,    51] loss : 1.637\n",
      "[14,    52] loss : 1.604\n",
      "[14,    53] loss : 1.623\n",
      "[14,    54] loss : 1.646\n",
      "[14,    55] loss : 1.620\n",
      "[14,    56] loss : 1.625\n",
      "[14,    57] loss : 1.629\n",
      "[14,    58] loss : 1.619\n",
      "[14,    59] loss : 1.616\n",
      "[14,    60] loss : 1.618\n",
      "[15,     1] loss : 1.622\n",
      "[15,     2] loss : 1.612\n",
      "[15,     3] loss : 1.607\n",
      "[15,     4] loss : 1.607\n",
      "[15,     5] loss : 1.613\n",
      "[15,     6] loss : 1.618\n",
      "[15,     7] loss : 1.617\n",
      "[15,     8] loss : 1.616\n",
      "[15,     9] loss : 1.613\n",
      "[15,    10] loss : 1.602\n",
      "[15,    11] loss : 1.592\n",
      "[15,    12] loss : 1.597\n",
      "[15,    13] loss : 1.591\n",
      "[15,    14] loss : 1.594\n",
      "[15,    15] loss : 1.628\n",
      "[15,    16] loss : 1.591\n",
      "[15,    17] loss : 1.586\n",
      "[15,    18] loss : 1.583\n",
      "[15,    19] loss : 1.585\n",
      "[15,    20] loss : 1.579\n",
      "[15,    21] loss : 1.586\n",
      "[15,    22] loss : 1.584\n",
      "[15,    23] loss : 1.555\n",
      "[15,    24] loss : 1.578\n",
      "[15,    25] loss : 1.589\n",
      "[15,    26] loss : 1.577\n",
      "[15,    27] loss : 1.581\n",
      "[15,    28] loss : 1.546\n",
      "[15,    29] loss : 1.575\n",
      "[15,    30] loss : 1.581\n",
      "[15,    31] loss : 1.585\n",
      "[15,    32] loss : 1.557\n",
      "[15,    33] loss : 1.557\n",
      "[15,    34] loss : 1.579\n",
      "[15,    35] loss : 1.572\n",
      "[15,    36] loss : 1.581\n",
      "[15,    37] loss : 1.539\n",
      "[15,    38] loss : 1.535\n",
      "[15,    39] loss : 1.548\n",
      "[15,    40] loss : 1.546\n",
      "[15,    41] loss : 1.562\n",
      "[15,    42] loss : 1.540\n",
      "[15,    43] loss : 1.537\n",
      "[15,    44] loss : 1.563\n",
      "[15,    45] loss : 1.553\n",
      "[15,    46] loss : 1.529\n",
      "[15,    47] loss : 1.547\n",
      "[15,    48] loss : 1.525\n",
      "[15,    49] loss : 1.519\n",
      "[15,    50] loss : 1.529\n",
      "[15,    51] loss : 1.543\n",
      "[15,    52] loss : 1.528\n",
      "[15,    53] loss : 1.522\n",
      "[15,    54] loss : 1.531\n",
      "[15,    55] loss : 1.527\n",
      "[15,    56] loss : 1.527\n",
      "[15,    57] loss : 1.512\n",
      "[15,    58] loss : 1.528\n",
      "[15,    59] loss : 1.508\n",
      "[15,    60] loss : 1.513\n",
      "[16,     1] loss : 1.514\n",
      "[16,     2] loss : 1.533\n",
      "[16,     3] loss : 1.505\n",
      "[16,     4] loss : 1.508\n",
      "[16,     5] loss : 1.526\n",
      "[16,     6] loss : 1.501\n",
      "[16,     7] loss : 1.512\n",
      "[16,     8] loss : 1.526\n",
      "[16,     9] loss : 1.547\n",
      "[16,    10] loss : 1.503\n",
      "[16,    11] loss : 1.490\n",
      "[16,    12] loss : 1.513\n",
      "[16,    13] loss : 1.472\n",
      "[16,    14] loss : 1.501\n",
      "[16,    15] loss : 1.500\n",
      "[16,    16] loss : 1.489\n",
      "[16,    17] loss : 1.489\n",
      "[16,    18] loss : 1.534\n",
      "[16,    19] loss : 1.498\n",
      "[16,    20] loss : 1.462\n",
      "[16,    21] loss : 1.499\n",
      "[16,    22] loss : 1.486\n",
      "[16,    23] loss : 1.460\n",
      "[16,    24] loss : 1.484\n",
      "[16,    25] loss : 1.475\n",
      "[16,    26] loss : 1.460\n",
      "[16,    27] loss : 1.462\n",
      "[16,    28] loss : 1.497\n",
      "[16,    29] loss : 1.469\n",
      "[16,    30] loss : 1.473\n",
      "[16,    31] loss : 1.471\n",
      "[16,    32] loss : 1.487\n",
      "[16,    33] loss : 1.482\n",
      "[16,    34] loss : 1.460\n",
      "[16,    35] loss : 1.451\n",
      "[16,    36] loss : 1.471\n",
      "[16,    37] loss : 1.455\n",
      "[16,    38] loss : 1.476\n",
      "[16,    39] loss : 1.464\n",
      "[16,    40] loss : 1.438\n",
      "[16,    41] loss : 1.439\n",
      "[16,    42] loss : 1.439\n",
      "[16,    43] loss : 1.440\n",
      "[16,    44] loss : 1.445\n",
      "[16,    45] loss : 1.438\n",
      "[16,    46] loss : 1.445\n",
      "[16,    47] loss : 1.458\n",
      "[16,    48] loss : 1.426\n",
      "[16,    49] loss : 1.452\n",
      "[16,    50] loss : 1.448\n",
      "[16,    51] loss : 1.409\n",
      "[16,    52] loss : 1.435\n",
      "[16,    53] loss : 1.421\n",
      "[16,    54] loss : 1.426\n",
      "[16,    55] loss : 1.417\n",
      "[16,    56] loss : 1.441\n",
      "[16,    57] loss : 1.422\n",
      "[16,    58] loss : 1.424\n",
      "[16,    59] loss : 1.422\n",
      "[16,    60] loss : 1.397\n",
      "[17,     1] loss : 1.412\n",
      "[17,     2] loss : 1.400\n",
      "[17,     3] loss : 1.421\n",
      "[17,     4] loss : 1.375\n",
      "[17,     5] loss : 1.427\n",
      "[17,     6] loss : 1.445\n",
      "[17,     7] loss : 1.412\n",
      "[17,     8] loss : 1.446\n",
      "[17,     9] loss : 1.395\n",
      "[17,    10] loss : 1.379\n",
      "[17,    11] loss : 1.410\n",
      "[17,    12] loss : 1.430\n",
      "[17,    13] loss : 1.390\n",
      "[17,    14] loss : 1.411\n",
      "[17,    15] loss : 1.377\n",
      "[17,    16] loss : 1.392\n",
      "[17,    17] loss : 1.347\n",
      "[17,    18] loss : 1.407\n",
      "[17,    19] loss : 1.404\n",
      "[17,    20] loss : 1.362\n",
      "[17,    21] loss : 1.405\n",
      "[17,    22] loss : 1.353\n",
      "[17,    23] loss : 1.396\n",
      "[17,    24] loss : 1.384\n",
      "[17,    25] loss : 1.378\n",
      "[17,    26] loss : 1.382\n",
      "[17,    27] loss : 1.359\n",
      "[17,    28] loss : 1.378\n",
      "[17,    29] loss : 1.368\n",
      "[17,    30] loss : 1.359\n",
      "[17,    31] loss : 1.374\n",
      "[17,    32] loss : 1.402\n",
      "[17,    33] loss : 1.371\n",
      "[17,    34] loss : 1.358\n",
      "[17,    35] loss : 1.351\n",
      "[17,    36] loss : 1.360\n",
      "[17,    37] loss : 1.365\n",
      "[17,    38] loss : 1.343\n",
      "[17,    39] loss : 1.334\n",
      "[17,    40] loss : 1.372\n",
      "[17,    41] loss : 1.370\n",
      "[17,    42] loss : 1.351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17,    43] loss : 1.349\n",
      "[17,    44] loss : 1.332\n",
      "[17,    45] loss : 1.356\n",
      "[17,    46] loss : 1.340\n",
      "[17,    47] loss : 1.343\n",
      "[17,    48] loss : 1.363\n",
      "[17,    49] loss : 1.347\n",
      "[17,    50] loss : 1.342\n",
      "[17,    51] loss : 1.339\n",
      "[17,    52] loss : 1.307\n",
      "[17,    53] loss : 1.330\n",
      "[17,    54] loss : 1.331\n",
      "[17,    55] loss : 1.338\n",
      "[17,    56] loss : 1.352\n",
      "[17,    57] loss : 1.341\n",
      "[17,    58] loss : 1.341\n",
      "[17,    59] loss : 1.309\n",
      "[17,    60] loss : 1.317\n",
      "[18,     1] loss : 1.330\n",
      "[18,     2] loss : 1.307\n",
      "[18,     3] loss : 1.304\n",
      "[18,     4] loss : 1.327\n",
      "[18,     5] loss : 1.335\n",
      "[18,     6] loss : 1.341\n",
      "[18,     7] loss : 1.298\n",
      "[18,     8] loss : 1.332\n",
      "[18,     9] loss : 1.294\n",
      "[18,    10] loss : 1.299\n",
      "[18,    11] loss : 1.269\n",
      "[18,    12] loss : 1.308\n",
      "[18,    13] loss : 1.306\n",
      "[18,    14] loss : 1.295\n",
      "[18,    15] loss : 1.294\n",
      "[18,    16] loss : 1.319\n",
      "[18,    17] loss : 1.322\n",
      "[18,    18] loss : 1.305\n",
      "[18,    19] loss : 1.300\n",
      "[18,    20] loss : 1.317\n",
      "[18,    21] loss : 1.256\n",
      "[18,    22] loss : 1.323\n",
      "[18,    23] loss : 1.293\n",
      "[18,    24] loss : 1.283\n",
      "[18,    25] loss : 1.272\n",
      "[18,    26] loss : 1.268\n",
      "[18,    27] loss : 1.295\n",
      "[18,    28] loss : 1.298\n",
      "[18,    29] loss : 1.264\n",
      "[18,    30] loss : 1.278\n",
      "[18,    31] loss : 1.249\n",
      "[18,    32] loss : 1.275\n",
      "[18,    33] loss : 1.248\n",
      "[18,    34] loss : 1.252\n",
      "[18,    35] loss : 1.254\n",
      "[18,    36] loss : 1.275\n",
      "[18,    37] loss : 1.252\n",
      "[18,    38] loss : 1.262\n",
      "[18,    39] loss : 1.296\n",
      "[18,    40] loss : 1.272\n",
      "[18,    41] loss : 1.268\n",
      "[18,    42] loss : 1.266\n",
      "[18,    43] loss : 1.255\n",
      "[18,    44] loss : 1.257\n",
      "[18,    45] loss : 1.266\n",
      "[18,    46] loss : 1.279\n",
      "[18,    47] loss : 1.266\n",
      "[18,    48] loss : 1.248\n",
      "[18,    49] loss : 1.255\n",
      "[18,    50] loss : 1.247\n",
      "[18,    51] loss : 1.257\n",
      "[18,    52] loss : 1.219\n",
      "[18,    53] loss : 1.238\n",
      "[18,    54] loss : 1.269\n",
      "[18,    55] loss : 1.212\n",
      "[18,    56] loss : 1.252\n",
      "[18,    57] loss : 1.246\n",
      "[18,    58] loss : 1.228\n",
      "[18,    59] loss : 1.248\n",
      "[18,    60] loss : 1.224\n",
      "[19,     1] loss : 1.189\n",
      "[19,     2] loss : 1.215\n",
      "[19,     3] loss : 1.217\n",
      "[19,     4] loss : 1.193\n",
      "[19,     5] loss : 1.232\n",
      "[19,     6] loss : 1.251\n",
      "[19,     7] loss : 1.235\n",
      "[19,     8] loss : 1.203\n",
      "[19,     9] loss : 1.230\n",
      "[19,    10] loss : 1.223\n",
      "[19,    11] loss : 1.190\n",
      "[19,    12] loss : 1.240\n",
      "[19,    13] loss : 1.180\n",
      "[19,    14] loss : 1.194\n",
      "[19,    15] loss : 1.204\n",
      "[19,    16] loss : 1.212\n",
      "[19,    17] loss : 1.202\n",
      "[19,    18] loss : 1.187\n",
      "[19,    19] loss : 1.234\n",
      "[19,    20] loss : 1.180\n",
      "[19,    21] loss : 1.207\n",
      "[19,    22] loss : 1.186\n",
      "[19,    23] loss : 1.200\n",
      "[19,    24] loss : 1.199\n",
      "[19,    25] loss : 1.165\n",
      "[19,    26] loss : 1.196\n",
      "[19,    27] loss : 1.186\n",
      "[19,    28] loss : 1.188\n",
      "[19,    29] loss : 1.208\n",
      "[19,    30] loss : 1.188\n",
      "[19,    31] loss : 1.215\n",
      "[19,    32] loss : 1.195\n",
      "[19,    33] loss : 1.199\n",
      "[19,    34] loss : 1.182\n",
      "[19,    35] loss : 1.214\n",
      "[19,    36] loss : 1.198\n",
      "[19,    37] loss : 1.192\n",
      "[19,    38] loss : 1.166\n",
      "[19,    39] loss : 1.153\n",
      "[19,    40] loss : 1.167\n",
      "[19,    41] loss : 1.197\n",
      "[19,    42] loss : 1.206\n",
      "[19,    43] loss : 1.151\n",
      "[19,    44] loss : 1.151\n",
      "[19,    45] loss : 1.160\n",
      "[19,    46] loss : 1.165\n",
      "[19,    47] loss : 1.170\n",
      "[19,    48] loss : 1.193\n",
      "[19,    49] loss : 1.189\n",
      "[19,    50] loss : 1.167\n",
      "[19,    51] loss : 1.142\n",
      "[19,    52] loss : 1.152\n",
      "[19,    53] loss : 1.176\n",
      "[19,    54] loss : 1.151\n",
      "[19,    55] loss : 1.160\n",
      "[19,    56] loss : 1.131\n",
      "[19,    57] loss : 1.163\n",
      "[19,    58] loss : 1.149\n",
      "[19,    59] loss : 1.135\n",
      "[19,    60] loss : 1.129\n",
      "[20,     1] loss : 1.143\n",
      "[20,     2] loss : 1.117\n",
      "[20,     3] loss : 1.114\n",
      "[20,     4] loss : 1.120\n",
      "[20,     5] loss : 1.153\n",
      "[20,     6] loss : 1.198\n",
      "[20,     7] loss : 1.158\n",
      "[20,     8] loss : 1.126\n",
      "[20,     9] loss : 1.124\n",
      "[20,    10] loss : 1.131\n",
      "[20,    11] loss : 1.113\n",
      "[20,    12] loss : 1.145\n",
      "[20,    13] loss : 1.134\n",
      "[20,    14] loss : 1.180\n",
      "[20,    15] loss : 1.125\n",
      "[20,    16] loss : 1.122\n",
      "[20,    17] loss : 1.141\n",
      "[20,    18] loss : 1.124\n",
      "[20,    19] loss : 1.103\n",
      "[20,    20] loss : 1.114\n",
      "[20,    21] loss : 1.119\n",
      "[20,    22] loss : 1.098\n",
      "[20,    23] loss : 1.111\n",
      "[20,    24] loss : 1.111\n",
      "[20,    25] loss : 1.114\n",
      "[20,    26] loss : 1.082\n",
      "[20,    27] loss : 1.114\n",
      "[20,    28] loss : 1.098\n",
      "[20,    29] loss : 1.107\n",
      "[20,    30] loss : 1.115\n",
      "[20,    31] loss : 1.125\n",
      "[20,    32] loss : 1.099\n",
      "[20,    33] loss : 1.138\n",
      "[20,    34] loss : 1.103\n",
      "[20,    35] loss : 1.089\n",
      "[20,    36] loss : 1.093\n",
      "[20,    37] loss : 1.089\n",
      "[20,    38] loss : 1.139\n",
      "[20,    39] loss : 1.106\n",
      "[20,    40] loss : 1.082\n",
      "[20,    41] loss : 1.087\n",
      "[20,    42] loss : 1.080\n",
      "[20,    43] loss : 1.109\n",
      "[20,    44] loss : 1.078\n",
      "[20,    45] loss : 1.068\n",
      "[20,    46] loss : 1.071\n",
      "[20,    47] loss : 1.051\n",
      "[20,    48] loss : 1.090\n",
      "[20,    49] loss : 1.078\n",
      "[20,    50] loss : 1.069\n",
      "[20,    51] loss : 1.072\n",
      "[20,    52] loss : 1.101\n",
      "[20,    53] loss : 1.092\n",
      "[20,    54] loss : 1.056\n",
      "[20,    55] loss : 1.108\n",
      "[20,    56] loss : 1.090\n",
      "[20,    57] loss : 1.082\n",
      "[20,    58] loss : 1.036\n",
      "[20,    59] loss : 1.065\n",
      "[20,    60] loss : 1.074\n",
      "[21,     1] loss : 1.052\n",
      "[21,     2] loss : 1.071\n",
      "[21,     3] loss : 1.051\n",
      "[21,     4] loss : 1.055\n",
      "[21,     5] loss : 1.053\n",
      "[21,     6] loss : 1.055\n",
      "[21,     7] loss : 1.021\n",
      "[21,     8] loss : 1.063\n",
      "[21,     9] loss : 1.016\n",
      "[21,    10] loss : 1.055\n",
      "[21,    11] loss : 1.017\n",
      "[21,    12] loss : 1.070\n",
      "[21,    13] loss : 1.113\n",
      "[21,    14] loss : 1.040\n",
      "[21,    15] loss : 1.065\n",
      "[21,    16] loss : 1.025\n",
      "[21,    17] loss : 1.087\n",
      "[21,    18] loss : 1.054\n",
      "[21,    19] loss : 1.063\n",
      "[21,    20] loss : 1.036\n",
      "[21,    21] loss : 1.025\n",
      "[21,    22] loss : 1.071\n",
      "[21,    23] loss : 1.030\n",
      "[21,    24] loss : 1.009\n",
      "[21,    25] loss : 1.017\n",
      "[21,    26] loss : 1.041\n",
      "[21,    27] loss : 1.038\n",
      "[21,    28] loss : 1.015\n",
      "[21,    29] loss : 1.068\n",
      "[21,    30] loss : 1.026\n",
      "[21,    31] loss : 1.003\n",
      "[21,    32] loss : 1.005\n",
      "[21,    33] loss : 1.040\n",
      "[21,    34] loss : 1.041\n",
      "[21,    35] loss : 1.026\n",
      "[21,    36] loss : 1.005\n",
      "[21,    37] loss : 0.998\n",
      "[21,    38] loss : 1.022\n",
      "[21,    39] loss : 1.028\n",
      "[21,    40] loss : 0.980\n",
      "[21,    41] loss : 1.041\n",
      "[21,    42] loss : 1.029\n",
      "[21,    43] loss : 1.010\n",
      "[21,    44] loss : 1.046\n",
      "[21,    45] loss : 1.007\n",
      "[21,    46] loss : 0.965\n",
      "[21,    47] loss : 0.981\n",
      "[21,    48] loss : 1.009\n",
      "[21,    49] loss : 0.999\n",
      "[21,    50] loss : 1.015\n",
      "[21,    51] loss : 1.018\n",
      "[21,    52] loss : 1.010\n",
      "[21,    53] loss : 0.995\n",
      "[21,    54] loss : 0.991\n",
      "[21,    55] loss : 0.979\n",
      "[21,    56] loss : 1.009\n",
      "[21,    57] loss : 1.017\n",
      "[21,    58] loss : 1.002\n",
      "[21,    59] loss : 0.995\n",
      "[21,    60] loss : 0.992\n",
      "[22,     1] loss : 0.995\n",
      "[22,     2] loss : 0.974\n",
      "[22,     3] loss : 1.003\n",
      "[22,     4] loss : 0.992\n",
      "[22,     5] loss : 0.968\n",
      "[22,     6] loss : 0.992\n",
      "[22,     7] loss : 0.980\n",
      "[22,     8] loss : 0.976\n",
      "[22,     9] loss : 0.959\n",
      "[22,    10] loss : 0.993\n",
      "[22,    11] loss : 0.979\n",
      "[22,    12] loss : 0.981\n",
      "[22,    13] loss : 0.956\n",
      "[22,    14] loss : 0.980\n",
      "[22,    15] loss : 0.959\n",
      "[22,    16] loss : 0.988\n",
      "[22,    17] loss : 0.994\n",
      "[22,    18] loss : 0.982\n",
      "[22,    19] loss : 0.999\n",
      "[22,    20] loss : 0.950\n",
      "[22,    21] loss : 0.929\n",
      "[22,    22] loss : 0.984\n",
      "[22,    23] loss : 0.969\n",
      "[22,    24] loss : 0.953\n",
      "[22,    25] loss : 0.975\n",
      "[22,    26] loss : 0.961\n",
      "[22,    27] loss : 0.992\n",
      "[22,    28] loss : 1.008\n",
      "[22,    29] loss : 0.989\n",
      "[22,    30] loss : 0.950\n",
      "[22,    31] loss : 0.985\n",
      "[22,    32] loss : 0.919\n",
      "[22,    33] loss : 0.967\n",
      "[22,    34] loss : 0.985\n",
      "[22,    35] loss : 0.928\n",
      "[22,    36] loss : 0.972\n",
      "[22,    37] loss : 0.940\n",
      "[22,    38] loss : 0.939\n",
      "[22,    39] loss : 0.975\n",
      "[22,    40] loss : 0.956\n",
      "[22,    41] loss : 0.960\n",
      "[22,    42] loss : 0.959\n",
      "[22,    43] loss : 0.954\n",
      "[22,    44] loss : 0.976\n",
      "[22,    45] loss : 0.936\n",
      "[22,    46] loss : 0.909\n",
      "[22,    47] loss : 0.952\n",
      "[22,    48] loss : 0.946\n",
      "[22,    49] loss : 0.940\n",
      "[22,    50] loss : 0.943\n",
      "[22,    51] loss : 0.947\n",
      "[22,    52] loss : 0.961\n",
      "[22,    53] loss : 0.941\n",
      "[22,    54] loss : 0.964\n",
      "[22,    55] loss : 0.926\n",
      "[22,    56] loss : 0.903\n",
      "[22,    57] loss : 0.954\n",
      "[22,    58] loss : 0.917\n",
      "[22,    59] loss : 0.924\n",
      "[22,    60] loss : 0.927\n",
      "[23,     1] loss : 0.916\n",
      "[23,     2] loss : 0.945\n",
      "[23,     3] loss : 0.950\n",
      "[23,     4] loss : 0.926\n",
      "[23,     5] loss : 0.896\n",
      "[23,     6] loss : 0.943\n",
      "[23,     7] loss : 0.898\n",
      "[23,     8] loss : 0.919\n",
      "[23,     9] loss : 0.918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23,    10] loss : 0.909\n",
      "[23,    11] loss : 0.908\n",
      "[23,    12] loss : 0.927\n",
      "[23,    13] loss : 0.906\n",
      "[23,    14] loss : 0.892\n",
      "[23,    15] loss : 0.905\n",
      "[23,    16] loss : 0.877\n",
      "[23,    17] loss : 0.888\n",
      "[23,    18] loss : 0.909\n",
      "[23,    19] loss : 0.899\n",
      "[23,    20] loss : 0.932\n",
      "[23,    21] loss : 0.883\n",
      "[23,    22] loss : 0.909\n",
      "[23,    23] loss : 0.920\n",
      "[23,    24] loss : 0.949\n",
      "[23,    25] loss : 0.916\n",
      "[23,    26] loss : 0.916\n",
      "[23,    27] loss : 0.884\n",
      "[23,    28] loss : 0.899\n",
      "[23,    29] loss : 0.907\n",
      "[23,    30] loss : 0.909\n",
      "[23,    31] loss : 0.871\n",
      "[23,    32] loss : 0.891\n",
      "[23,    33] loss : 0.899\n",
      "[23,    34] loss : 0.897\n",
      "[23,    35] loss : 0.921\n",
      "[23,    36] loss : 0.901\n",
      "[23,    37] loss : 0.895\n",
      "[23,    38] loss : 0.910\n",
      "[23,    39] loss : 0.884\n",
      "[23,    40] loss : 0.863\n",
      "[23,    41] loss : 0.846\n",
      "[23,    42] loss : 0.917\n",
      "[23,    43] loss : 0.893\n",
      "[23,    44] loss : 0.875\n",
      "[23,    45] loss : 0.886\n",
      "[23,    46] loss : 0.872\n",
      "[23,    47] loss : 0.879\n",
      "[23,    48] loss : 0.890\n",
      "[23,    49] loss : 0.888\n",
      "[23,    50] loss : 0.885\n",
      "[23,    51] loss : 0.877\n",
      "[23,    52] loss : 0.854\n",
      "[23,    53] loss : 0.904\n",
      "[23,    54] loss : 0.871\n",
      "[23,    55] loss : 0.881\n",
      "[23,    56] loss : 0.896\n",
      "[23,    57] loss : 0.929\n",
      "[23,    58] loss : 0.873\n",
      "[23,    59] loss : 0.851\n",
      "[23,    60] loss : 0.897\n",
      "[24,     1] loss : 0.882\n",
      "[24,     2] loss : 0.890\n",
      "[24,     3] loss : 0.844\n",
      "[24,     4] loss : 0.862\n",
      "[24,     5] loss : 0.872\n",
      "[24,     6] loss : 0.865\n",
      "[24,     7] loss : 0.896\n",
      "[24,     8] loss : 0.869\n",
      "[24,     9] loss : 0.857\n",
      "[24,    10] loss : 0.856\n",
      "[24,    11] loss : 0.857\n",
      "[24,    12] loss : 0.875\n",
      "[24,    13] loss : 0.843\n",
      "[24,    14] loss : 0.846\n",
      "[24,    15] loss : 0.837\n",
      "[24,    16] loss : 0.897\n",
      "[24,    17] loss : 0.886\n",
      "[24,    18] loss : 0.908\n",
      "[24,    19] loss : 0.878\n",
      "[24,    20] loss : 0.887\n",
      "[24,    21] loss : 0.867\n",
      "[24,    22] loss : 0.872\n",
      "[24,    23] loss : 0.834\n",
      "[24,    24] loss : 0.846\n",
      "[24,    25] loss : 0.818\n",
      "[24,    26] loss : 0.837\n",
      "[24,    27] loss : 0.864\n",
      "[24,    28] loss : 0.851\n",
      "[24,    29] loss : 0.839\n",
      "[24,    30] loss : 0.820\n",
      "[24,    31] loss : 0.868\n",
      "[24,    32] loss : 0.849\n",
      "[24,    33] loss : 0.838\n",
      "[24,    34] loss : 0.848\n",
      "[24,    35] loss : 0.801\n",
      "[24,    36] loss : 0.858\n",
      "[24,    37] loss : 0.818\n",
      "[24,    38] loss : 0.844\n",
      "[24,    39] loss : 0.848\n",
      "[24,    40] loss : 0.849\n",
      "[24,    41] loss : 0.812\n",
      "[24,    42] loss : 0.842\n",
      "[24,    43] loss : 0.846\n",
      "[24,    44] loss : 0.875\n",
      "[24,    45] loss : 0.793\n",
      "[24,    46] loss : 0.805\n",
      "[24,    47] loss : 0.841\n",
      "[24,    48] loss : 0.836\n",
      "[24,    49] loss : 0.841\n",
      "[24,    50] loss : 0.823\n",
      "[24,    51] loss : 0.808\n",
      "[24,    52] loss : 0.845\n",
      "[24,    53] loss : 0.880\n",
      "[24,    54] loss : 0.848\n",
      "[24,    55] loss : 0.858\n",
      "[24,    56] loss : 0.837\n",
      "[24,    57] loss : 0.813\n",
      "[24,    58] loss : 0.831\n",
      "[24,    59] loss : 0.812\n",
      "[24,    60] loss : 0.812\n",
      "[25,     1] loss : 0.817\n",
      "[25,     2] loss : 0.815\n",
      "[25,     3] loss : 0.830\n",
      "[25,     4] loss : 0.776\n",
      "[25,     5] loss : 0.808\n",
      "[25,     6] loss : 0.837\n",
      "[25,     7] loss : 0.857\n",
      "[25,     8] loss : 0.818\n",
      "[25,     9] loss : 0.841\n",
      "[25,    10] loss : 0.780\n",
      "[25,    11] loss : 0.791\n",
      "[25,    12] loss : 0.811\n",
      "[25,    13] loss : 0.844\n",
      "[25,    14] loss : 0.800\n",
      "[25,    15] loss : 0.827\n",
      "[25,    16] loss : 0.836\n",
      "[25,    17] loss : 0.843\n",
      "[25,    18] loss : 0.808\n",
      "[25,    19] loss : 0.795\n",
      "[25,    20] loss : 0.810\n",
      "[25,    21] loss : 0.807\n",
      "[25,    22] loss : 0.779\n",
      "[25,    23] loss : 0.818\n",
      "[25,    24] loss : 0.816\n",
      "[25,    25] loss : 0.795\n",
      "[25,    26] loss : 0.818\n",
      "[25,    27] loss : 0.827\n",
      "[25,    28] loss : 0.812\n",
      "[25,    29] loss : 0.757\n",
      "[25,    30] loss : 0.802\n",
      "[25,    31] loss : 0.849\n",
      "[25,    32] loss : 0.779\n",
      "[25,    33] loss : 0.780\n",
      "[25,    34] loss : 0.770\n",
      "[25,    35] loss : 0.773\n",
      "[25,    36] loss : 0.815\n",
      "[25,    37] loss : 0.806\n",
      "[25,    38] loss : 0.833\n",
      "[25,    39] loss : 0.759\n",
      "[25,    40] loss : 0.802\n",
      "[25,    41] loss : 0.812\n",
      "[25,    42] loss : 0.805\n",
      "[25,    43] loss : 0.801\n",
      "[25,    44] loss : 0.797\n",
      "[25,    45] loss : 0.812\n",
      "[25,    46] loss : 0.771\n",
      "[25,    47] loss : 0.804\n",
      "[25,    48] loss : 0.810\n",
      "[25,    49] loss : 0.797\n",
      "[25,    50] loss : 0.725\n",
      "[25,    51] loss : 0.805\n",
      "[25,    52] loss : 0.821\n",
      "[25,    53] loss : 0.796\n",
      "[25,    54] loss : 0.784\n",
      "[25,    55] loss : 0.770\n",
      "[25,    56] loss : 0.804\n",
      "[25,    57] loss : 0.751\n",
      "[25,    58] loss : 0.750\n",
      "[25,    59] loss : 0.775\n",
      "[25,    60] loss : 0.778\n",
      "[26,     1] loss : 0.807\n",
      "[26,     2] loss : 0.765\n",
      "[26,     3] loss : 0.764\n",
      "[26,     4] loss : 0.784\n",
      "[26,     5] loss : 0.760\n",
      "[26,     6] loss : 0.770\n",
      "[26,     7] loss : 0.782\n",
      "[26,     8] loss : 0.737\n",
      "[26,     9] loss : 0.790\n",
      "[26,    10] loss : 0.738\n",
      "[26,    11] loss : 0.752\n",
      "[26,    12] loss : 0.770\n",
      "[26,    13] loss : 0.771\n",
      "[26,    14] loss : 0.750\n",
      "[26,    15] loss : 0.767\n",
      "[26,    16] loss : 0.756\n",
      "[26,    17] loss : 0.744\n",
      "[26,    18] loss : 0.761\n",
      "[26,    19] loss : 0.806\n",
      "[26,    20] loss : 0.790\n",
      "[26,    21] loss : 0.753\n",
      "[26,    22] loss : 0.753\n",
      "[26,    23] loss : 0.739\n",
      "[26,    24] loss : 0.793\n",
      "[26,    25] loss : 0.777\n",
      "[26,    26] loss : 0.786\n",
      "[26,    27] loss : 0.759\n",
      "[26,    28] loss : 0.777\n",
      "[26,    29] loss : 0.735\n",
      "[26,    30] loss : 0.755\n",
      "[26,    31] loss : 0.746\n",
      "[26,    32] loss : 0.738\n",
      "[26,    33] loss : 0.744\n",
      "[26,    34] loss : 0.764\n",
      "[26,    35] loss : 0.762\n",
      "[26,    36] loss : 0.734\n",
      "[26,    37] loss : 0.709\n",
      "[26,    38] loss : 0.777\n",
      "[26,    39] loss : 0.758\n",
      "[26,    40] loss : 0.743\n",
      "[26,    41] loss : 0.772\n",
      "[26,    42] loss : 0.751\n",
      "[26,    43] loss : 0.751\n",
      "[26,    44] loss : 0.716\n",
      "[26,    45] loss : 0.785\n",
      "[26,    46] loss : 0.755\n",
      "[26,    47] loss : 0.761\n",
      "[26,    48] loss : 0.725\n",
      "[26,    49] loss : 0.742\n",
      "[26,    50] loss : 0.761\n",
      "[26,    51] loss : 0.755\n",
      "[26,    52] loss : 0.762\n",
      "[26,    53] loss : 0.742\n",
      "[26,    54] loss : 0.732\n",
      "[26,    55] loss : 0.766\n",
      "[26,    56] loss : 0.766\n",
      "[26,    57] loss : 0.722\n",
      "[26,    58] loss : 0.735\n",
      "[26,    59] loss : 0.744\n",
      "[26,    60] loss : 0.744\n",
      "[27,     1] loss : 0.725\n",
      "[27,     2] loss : 0.755\n",
      "[27,     3] loss : 0.734\n",
      "[27,     4] loss : 0.772\n",
      "[27,     5] loss : 0.751\n",
      "[27,     6] loss : 0.708\n",
      "[27,     7] loss : 0.736\n",
      "[27,     8] loss : 0.765\n",
      "[27,     9] loss : 0.712\n",
      "[27,    10] loss : 0.745\n",
      "[27,    11] loss : 0.751\n",
      "[27,    12] loss : 0.730\n",
      "[27,    13] loss : 0.712\n",
      "[27,    14] loss : 0.707\n",
      "[27,    15] loss : 0.693\n",
      "[27,    16] loss : 0.716\n",
      "[27,    17] loss : 0.730\n",
      "[27,    18] loss : 0.724\n",
      "[27,    19] loss : 0.708\n",
      "[27,    20] loss : 0.718\n",
      "[27,    21] loss : 0.719\n",
      "[27,    22] loss : 0.703\n",
      "[27,    23] loss : 0.698\n",
      "[27,    24] loss : 0.725\n",
      "[27,    25] loss : 0.730\n",
      "[27,    26] loss : 0.699\n",
      "[27,    27] loss : 0.719\n",
      "[27,    28] loss : 0.718\n",
      "[27,    29] loss : 0.724\n",
      "[27,    30] loss : 0.709\n",
      "[27,    31] loss : 0.740\n",
      "[27,    32] loss : 0.718\n",
      "[27,    33] loss : 0.729\n",
      "[27,    34] loss : 0.706\n",
      "[27,    35] loss : 0.739\n",
      "[27,    36] loss : 0.720\n",
      "[27,    37] loss : 0.754\n",
      "[27,    38] loss : 0.719\n",
      "[27,    39] loss : 0.728\n",
      "[27,    40] loss : 0.732\n",
      "[27,    41] loss : 0.714\n",
      "[27,    42] loss : 0.715\n",
      "[27,    43] loss : 0.729\n",
      "[27,    44] loss : 0.715\n",
      "[27,    45] loss : 0.702\n",
      "[27,    46] loss : 0.652\n",
      "[27,    47] loss : 0.675\n",
      "[27,    48] loss : 0.705\n",
      "[27,    49] loss : 0.717\n",
      "[27,    50] loss : 0.703\n",
      "[27,    51] loss : 0.713\n",
      "[27,    52] loss : 0.697\n",
      "[27,    53] loss : 0.715\n",
      "[27,    54] loss : 0.709\n",
      "[27,    55] loss : 0.685\n",
      "[27,    56] loss : 0.697\n",
      "[27,    57] loss : 0.689\n",
      "[27,    58] loss : 0.734\n",
      "[27,    59] loss : 0.717\n",
      "[27,    60] loss : 0.667\n",
      "[28,     1] loss : 0.688\n",
      "[28,     2] loss : 0.715\n",
      "[28,     3] loss : 0.696\n",
      "[28,     4] loss : 0.704\n",
      "[28,     5] loss : 0.725\n",
      "[28,     6] loss : 0.723\n",
      "[28,     7] loss : 0.698\n",
      "[28,     8] loss : 0.711\n",
      "[28,     9] loss : 0.714\n",
      "[28,    10] loss : 0.679\n",
      "[28,    11] loss : 0.666\n",
      "[28,    12] loss : 0.681\n",
      "[28,    13] loss : 0.718\n",
      "[28,    14] loss : 0.706\n",
      "[28,    15] loss : 0.735\n",
      "[28,    16] loss : 0.688\n",
      "[28,    17] loss : 0.693\n",
      "[28,    18] loss : 0.678\n",
      "[28,    19] loss : 0.685\n",
      "[28,    20] loss : 0.689\n",
      "[28,    21] loss : 0.691\n",
      "[28,    22] loss : 0.725\n",
      "[28,    23] loss : 0.682\n",
      "[28,    24] loss : 0.695\n",
      "[28,    25] loss : 0.688\n",
      "[28,    26] loss : 0.657\n",
      "[28,    27] loss : 0.687\n",
      "[28,    28] loss : 0.696\n",
      "[28,    29] loss : 0.712\n",
      "[28,    30] loss : 0.710\n",
      "[28,    31] loss : 0.661\n",
      "[28,    32] loss : 0.695\n",
      "[28,    33] loss : 0.659\n",
      "[28,    34] loss : 0.648\n",
      "[28,    35] loss : 0.731\n",
      "[28,    36] loss : 0.667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28,    37] loss : 0.698\n",
      "[28,    38] loss : 0.670\n",
      "[28,    39] loss : 0.691\n",
      "[28,    40] loss : 0.631\n",
      "[28,    41] loss : 0.694\n",
      "[28,    42] loss : 0.689\n",
      "[28,    43] loss : 0.679\n",
      "[28,    44] loss : 0.686\n",
      "[28,    45] loss : 0.737\n",
      "[28,    46] loss : 0.659\n",
      "[28,    47] loss : 0.728\n",
      "[28,    48] loss : 0.663\n",
      "[28,    49] loss : 0.695\n",
      "[28,    50] loss : 0.694\n",
      "[28,    51] loss : 0.657\n",
      "[28,    52] loss : 0.672\n",
      "[28,    53] loss : 0.635\n",
      "[28,    54] loss : 0.639\n",
      "[28,    55] loss : 0.673\n",
      "[28,    56] loss : 0.662\n",
      "[28,    57] loss : 0.670\n",
      "[28,    58] loss : 0.653\n",
      "[28,    59] loss : 0.633\n",
      "[28,    60] loss : 0.682\n",
      "[29,     1] loss : 0.670\n",
      "[29,     2] loss : 0.653\n",
      "[29,     3] loss : 0.641\n",
      "[29,     4] loss : 0.671\n",
      "[29,     5] loss : 0.655\n",
      "[29,     6] loss : 0.669\n",
      "[29,     7] loss : 0.665\n",
      "[29,     8] loss : 0.651\n",
      "[29,     9] loss : 0.666\n",
      "[29,    10] loss : 0.657\n",
      "[29,    11] loss : 0.686\n",
      "[29,    12] loss : 0.668\n",
      "[29,    13] loss : 0.694\n",
      "[29,    14] loss : 0.663\n",
      "[29,    15] loss : 0.658\n",
      "[29,    16] loss : 0.677\n",
      "[29,    17] loss : 0.653\n",
      "[29,    18] loss : 0.650\n",
      "[29,    19] loss : 0.696\n",
      "[29,    20] loss : 0.654\n",
      "[29,    21] loss : 0.622\n",
      "[29,    22] loss : 0.647\n",
      "[29,    23] loss : 0.616\n",
      "[29,    24] loss : 0.683\n",
      "[29,    25] loss : 0.657\n",
      "[29,    26] loss : 0.667\n",
      "[29,    27] loss : 0.641\n",
      "[29,    28] loss : 0.697\n",
      "[29,    29] loss : 0.635\n",
      "[29,    30] loss : 0.633\n",
      "[29,    31] loss : 0.623\n",
      "[29,    32] loss : 0.648\n",
      "[29,    33] loss : 0.668\n",
      "[29,    34] loss : 0.684\n",
      "[29,    35] loss : 0.631\n",
      "[29,    36] loss : 0.679\n",
      "[29,    37] loss : 0.635\n",
      "[29,    38] loss : 0.661\n",
      "[29,    39] loss : 0.665\n",
      "[29,    40] loss : 0.656\n",
      "[29,    41] loss : 0.622\n",
      "[29,    42] loss : 0.680\n",
      "[29,    43] loss : 0.646\n",
      "[29,    44] loss : 0.654\n",
      "[29,    45] loss : 0.635\n",
      "[29,    46] loss : 0.666\n",
      "[29,    47] loss : 0.663\n",
      "[29,    48] loss : 0.661\n",
      "[29,    49] loss : 0.626\n",
      "[29,    50] loss : 0.628\n",
      "[29,    51] loss : 0.662\n",
      "[29,    52] loss : 0.622\n",
      "[29,    53] loss : 0.629\n",
      "[29,    54] loss : 0.626\n",
      "[29,    55] loss : 0.615\n",
      "[29,    56] loss : 0.627\n",
      "[29,    57] loss : 0.707\n",
      "[29,    58] loss : 0.662\n",
      "[29,    59] loss : 0.667\n",
      "[29,    60] loss : 0.640\n",
      "[30,     1] loss : 0.634\n",
      "[30,     2] loss : 0.664\n",
      "[30,     3] loss : 0.644\n",
      "[30,     4] loss : 0.615\n",
      "[30,     5] loss : 0.624\n",
      "[30,     6] loss : 0.615\n",
      "[30,     7] loss : 0.623\n",
      "[30,     8] loss : 0.671\n",
      "[30,     9] loss : 0.629\n",
      "[30,    10] loss : 0.611\n",
      "[30,    11] loss : 0.625\n",
      "[30,    12] loss : 0.647\n",
      "[30,    13] loss : 0.612\n",
      "[30,    14] loss : 0.610\n",
      "[30,    15] loss : 0.635\n",
      "[30,    16] loss : 0.626\n",
      "[30,    17] loss : 0.648\n",
      "[30,    18] loss : 0.644\n",
      "[30,    19] loss : 0.640\n",
      "[30,    20] loss : 0.612\n",
      "[30,    21] loss : 0.652\n",
      "[30,    22] loss : 0.609\n",
      "[30,    23] loss : 0.634\n",
      "[30,    24] loss : 0.635\n",
      "[30,    25] loss : 0.638\n",
      "[30,    26] loss : 0.593\n",
      "[30,    27] loss : 0.580\n",
      "[30,    28] loss : 0.639\n",
      "[30,    29] loss : 0.633\n",
      "[30,    30] loss : 0.589\n",
      "[30,    31] loss : 0.620\n",
      "[30,    32] loss : 0.634\n",
      "[30,    33] loss : 0.644\n",
      "[30,    34] loss : 0.652\n",
      "[30,    35] loss : 0.637\n",
      "[30,    36] loss : 0.637\n",
      "[30,    37] loss : 0.634\n",
      "[30,    38] loss : 0.615\n",
      "[30,    39] loss : 0.601\n",
      "[30,    40] loss : 0.599\n",
      "[30,    41] loss : 0.607\n",
      "[30,    42] loss : 0.606\n",
      "[30,    43] loss : 0.669\n",
      "[30,    44] loss : 0.578\n",
      "[30,    45] loss : 0.623\n",
      "[30,    46] loss : 0.613\n",
      "[30,    47] loss : 0.651\n",
      "[30,    48] loss : 0.606\n",
      "[30,    49] loss : 0.600\n",
      "[30,    50] loss : 0.607\n",
      "[30,    51] loss : 0.643\n",
      "[30,    52] loss : 0.608\n",
      "[30,    53] loss : 0.625\n",
      "[30,    54] loss : 0.625\n",
      "[30,    55] loss : 0.635\n",
      "[30,    56] loss : 0.631\n",
      "[30,    57] loss : 0.651\n",
      "[30,    58] loss : 0.571\n",
      "[30,    59] loss : 0.616\n",
      "[30,    60] loss : 0.606\n",
      "==> Finished Training ...\n"
     ]
    }
   ],
   "source": [
    "classifier.to(DEVICE)\n",
    "optimizer\n",
    "criterion.to(DEVICE)\n",
    "\n",
    "print('==> Start Training ...')\n",
    "\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(30):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs = data[0].float().to(DEVICE)\n",
    "        labels = data[1].to(DEVICE)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = classifier(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        prediction = outputs.data.max(1)[1]\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "#         if i%1000 == 999: #1000 mini batch  \n",
    "#             print('[%d, %5d] loss : %.3f'% ( epoch+1, i+1, running_loss/1000))\n",
    "#             train_loss.append(running_loss/1000)\n",
    "#             running_loss = 0.0\n",
    "        print('[%d, %5d] loss : %.3f'% ( epoch+1, i+1, running_loss))\n",
    "        train_loss.append(running_loss)\n",
    "        running_loss = 0.0\n",
    "    \n",
    "    \n",
    "print('==> Finished Training ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16bf1cfa508>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(train_loss)), train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True,\n",
    "                                         num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 85 %\n",
      "Accuracy of     0 : 91 %\n",
      "Accuracy of     1 : 97 %\n",
      "Accuracy of     2 : 83 %\n",
      "Accuracy of     3 : 80 %\n",
      "Accuracy of     4 : 90 %\n",
      "Accuracy of     5 : 73 %\n",
      "Accuracy of     6 : 89 %\n",
      "Accuracy of     7 : 87 %\n",
      "Accuracy of     8 : 69 %\n",
      "Accuracy of     9 : 84 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images = data[0].float().to(DEVICE)\n",
    "        labels = data[1].to(DEVICE)\n",
    "        outputs = classifier(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        i, 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
