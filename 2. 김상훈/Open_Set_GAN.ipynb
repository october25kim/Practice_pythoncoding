{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_G(z_dim, use_bias=False, print_summary=True):\n",
    "    kernel_init = 'glorot_normal'\n",
    "\n",
    "    z_in = Input(shape=(z_dim,))\n",
    "\n",
    "    net = Dense(units=128, kernel_regularizer=l2(1E-5), use_bias=use_bias)(z_in)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = LeakyReLU(alpha=0.15)(net)\n",
    "\n",
    "    net = Dense(units=7*7*32, kernel_regularizer=l2(1E-5), use_bias=use_bias)(net)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = LeakyReLU(alpha=0.15)(net)\n",
    "    net = Reshape((7, 7, 32))(net)\n",
    "\n",
    "    net = Conv2DTranspose(filters=16, kernel_size=(5, 5), strides=(2, 2), use_bias=use_bias,\n",
    "                          kernel_regularizer=l2(1E-5), padding='same', kernel_initializer=kernel_init)(net)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = LeakyReLU(alpha=0.15)(net)\n",
    "\n",
    "    out = Conv2DTranspose(filters=1, kernel_size=(5, 5), strides=(2, 2), use_bias=use_bias,\n",
    "                          padding='same', kernel_initializer=kernel_init, activation='tanh')(net)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=z_in, outputs=out, name='Generator')\n",
    "\n",
    "    if print_summary:\n",
    "        print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "def define_C(x_shape, n_classes, use_bias=True, print_summary=True):\n",
    "    kernel_init = 'glorot_normal'\n",
    "\n",
    "    x_in = Input(shape=x_shape)\n",
    "\n",
    "    net = Conv2D(filters=16, kernel_size=(5, 5), strides=(2, 2), use_bias=use_bias,\n",
    "                   kernel_regularizer=l2(1E-5), padding='same', kernel_initializer=kernel_init)(x_in)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = LeakyReLU(alpha=0.15)(net)\n",
    "\n",
    "    net = Conv2D(filters=32, kernel_size=(5, 5), strides=(2, 2), use_bias=use_bias,\n",
    "                   kernel_regularizer=l2(1E-5), padding='same', kernel_initializer=kernel_init)(net)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = Flatten()(net)\n",
    "\n",
    "    net = Dense(units=128, use_bias=use_bias)(net)\n",
    "    out = Dense(units=n_classes, activation='softmax', use_bias=use_bias)(net)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=x_in, outputs=out, name='Classifier')\n",
    "\n",
    "    if print_summary:\n",
    "        print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    z = tf.random.uniform(minval=-1, maxval=1, shape=(BATCH_SIZE, z_dim))\n",
    "    # 추가\n",
    "#     u = tf.ones([BATCH_SIZE,10])/10    \n",
    "    # 추가\n",
    "    with tf.GradientTape() as C_tape, tf.GradientTape() as G_tape:\n",
    "        Gz = generator(z, training=True)\n",
    "\n",
    "        C_real = classifier(x, training=True)\n",
    "        C_fake = classifier(Gz, training=True)\n",
    "\n",
    "        C_real_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, C_real))\n",
    "        # 추가\n",
    "#         kl = tf.keras.losses.KLDivergence()\n",
    "#         C_fake_loss = kl(u, C_fake) * (-1) # uniform 안되도록 kl(u, C_fake)는 uniform 될수록 작아져\n",
    "#         C_loss = C_real_loss + C_fake_loss\n",
    "\n",
    "        C_fake_loss = tf.reduce_mean((-1) * C_fake * tf.math.log(C_fake+1e-6))\n",
    "        C_loss = C_real_loss - C_fake_loss\n",
    "\n",
    "    C_grad = C_tape.gradient(C_loss, classifier.trainable_variables)\n",
    "    G_grad = G_tape.gradient(C_fake_loss, generator.trainable_variables)\n",
    "\n",
    "    C_opt.apply_gradients(zip(C_grad, classifier.trainable_variables))\n",
    "    G_opt.apply_gradients(zip(G_grad, generator.trainable_variables))\n",
    "\n",
    "    return C_real_loss, C_fake_loss\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    pbar = tqdm(range(epochs))\n",
    "    losses = []\n",
    "    for epoch in pbar:\n",
    "        for batch in dataset:\n",
    "            #print(batch[0].shape, batch[1].shape)\n",
    "            _loss = train_step(x=batch[0], y=batch[1])\n",
    "            losses.append(_loss)\n",
    "        pbar.set_description(\n",
    "                '[Joint Training of Classifier & Generator] | C-CrossEnt: {:.4f} | G-Ent: {:.4f}'.format(\n",
    "                        *np.array(losses).mean(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 1024\n",
    "z_dim = 64\n",
    "\n",
    "(train_images, train_classes), (test_images, test_classes) = load_data()\n",
    "train_images, test_images = train_images.astype(np.float32)/255, test_images.astype(np.float32)/255\n",
    "train_images, test_images = train_images*2 - 1, test_images*2 - 1\n",
    "train_images, test_images = np.expand_dims(train_images, 3), np.expand_dims(test_images, 3)\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "train_labels = enc.fit_transform(train_classes.reshape(-1,1))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE, 0, True).batch(BATCH_SIZE)\n",
    "\n",
    "generator = define_G(z_dim, True, False)\n",
    "classifier = define_C((28, 28, 1), 10, True, False)\n",
    "\n",
    "C_opt = tf.keras.optimizers.Adam(learning_rate=1E-4, beta_1=0.5)\n",
    "G_opt = tf.keras.optimizers.Adam(learning_rate=1E-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save_weights(\"generator_kl_epoch_{}.h5\".format(100))\n",
    "classifier.save_weights(\"classifier_kl_epoch_{}.h5\".format(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(c_train_images, c_train_classes), (c_test_images, c_test_classes) = tf.keras.datasets.cifar10.load_data()\n",
    "c_train_images, c_test_images = c_train_images.astype(np.float32)/255, c_test_images.astype(np.float32)/255\n",
    "c_train_images, c_test_images = c_train_images*2 - 1, c_test_images*2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_cifar10 = []\n",
    "for img in c_test_images:\n",
    "    gray_img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "    resized_cifar10.append(cv2.resize(gray_img, (28, 28), interpolation=cv2.INTER_CUBIC))\n",
    "cifar10_images = np.array(resized_cifar10)\n",
    "cifar10_images = np.expand_dims(cifar10_images, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.load_weights(\"generator_kl_epoch_100.h5\")\n",
    "# classifier.load_weights(\"classifier_kl_epoch_100.h5\")\n",
    "generator.load_weights(\"generator_epoch_500.h5\")\n",
    "classifier.load_weights(\"classifier_epoch_500.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.uniform(-1,1,(10000, 64))\n",
    "generated_images = generator.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = 10, 10\n",
    "figure, axes = plt.subplots(rows, columns)\n",
    "for row in range(rows):\n",
    "    for col in range(columns):\n",
    "        image = generated_images[row + rows * col, :, :, :]\n",
    "        axes[row, col].imshow(np.squeeze(image), cmap='gray')\n",
    "        axes[row, col].axis('off')\n",
    "figure.savefig('C:/GAN/venv/images/{}.png'.format('test1'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result1 = classifier.predict(generated_images)\n",
    "test_result2 = classifier.predict(cifar10_images)\n",
    "test_result3 = classifier.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels = np.argmax(test_result3,1)\n",
    "accuracy_score(test_classes,predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.ones(10000)\n",
    "len(arr[np.max(test_result2,1) < 0.99999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_Conditional_G(z_dim, n_classes=10, use_bias=False, print_summary=True):\n",
    "    kernel_init = 'glorot_normal'\n",
    "    \n",
    "    in_label = Input(shape=(1,))\n",
    "\t# embedding for categorical input\n",
    "\tli = Embedding(n_classes, 50)(in_label)\n",
    "\t# linear multiplication\n",
    "\tn_nodes = 7 * 7\n",
    "\tli = Dense(n_nodes)(li)\n",
    "\t# reshape to additional channel\n",
    "\tli = Reshape((7, 7, 1))(li)\n",
    "    z_in = Input(shape=(z_dim,))\n",
    "\n",
    "    net = Dense(units=128, kernel_regularizer=l2(1E-5), use_bias=use_bias)(z_in)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = LeakyReLU(alpha=0.15)(net)\n",
    "   \n",
    "    net = Dense(units=7*7*32, kernel_regularizer=l2(1E-5), use_bias=use_bias)(net)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = LeakyReLU(alpha=0.15)(net)\n",
    "    net = Reshape((7, 7, 32))(net)\n",
    "    \n",
    "    merge = Concatenate()([net,li])\n",
    "    \n",
    "    net = Conv2DTranspose(filters=16, kernel_size=(5, 5), strides=(2, 2), use_bias=use_bias,\n",
    "                          kernel_regularizer=l2(1E-5), padding='same', kernel_initializer=kernel_init)(merge)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = LeakyReLU(alpha=0.15)(net)\n",
    "\n",
    "    out = Conv2DTranspose(filters=1, kernel_size=(5, 5), strides=(2, 2), use_bias=use_bias,\n",
    "                          padding='same', kernel_initializer=kernel_init, activation='tanh')(net)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[z_in,in_label], outputs=out, name='Generator')\n",
    "\n",
    "    if print_summary:\n",
    "        print(model.summary())\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
