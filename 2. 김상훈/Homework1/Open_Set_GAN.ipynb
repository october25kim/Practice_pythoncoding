{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6797432121459597439\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6349818901446752382\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11453590006851914342\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x294801af4c8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tf.device(\"/xka_gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_G(z_dim, use_bias=False, print_summary=True):\n",
    "    kernel_init = 'glorot_normal'\n",
    "\n",
    "    z_in = Input(shape=(z_dim,))\n",
    "\n",
    "    net = Dense(units=128, kernel_regularizer=l2(1E-5), use_bias=use_bias)(z_in)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = LeakyReLU(alpha=0.15)(net)\n",
    "\n",
    "    net = Dense(units=7*7*32, kernel_regularizer=l2(1E-5), use_bias=use_bias)(net)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = LeakyReLU(alpha=0.15)(net)\n",
    "    net = Reshape((7, 7, 32))(net)\n",
    "\n",
    "    net = Conv2DTranspose(filters=16, kernel_size=(5, 5), strides=(2, 2), use_bias=use_bias,\n",
    "                          kernel_regularizer=l2(1E-5), padding='same', kernel_initializer=kernel_init)(net)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = LeakyReLU(alpha=0.15)(net)\n",
    "\n",
    "    out = Conv2DTranspose(filters=1, kernel_size=(5, 5), strides=(2, 2), use_bias=use_bias,\n",
    "                          padding='same', kernel_initializer=kernel_init, activation='tanh')(net)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=z_in, outputs=out, name='Generator')\n",
    "\n",
    "    if print_summary:\n",
    "        print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "def define_C(x_shape, n_classes, use_bias=True, print_summary=True):\n",
    "    kernel_init = 'glorot_normal'\n",
    "\n",
    "    x_in = Input(shape=x_shape)\n",
    "\n",
    "    net = Conv2D(filters=16, kernel_size=(5, 5), strides=(2, 2), use_bias=use_bias,\n",
    "                   kernel_regularizer=l2(1E-5), padding='same', kernel_initializer=kernel_init)(x_in)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = LeakyReLU(alpha=0.15)(net)\n",
    "\n",
    "    net = Conv2D(filters=32, kernel_size=(5, 5), strides=(2, 2), use_bias=use_bias,\n",
    "                   kernel_regularizer=l2(1E-5), padding='same', kernel_initializer=kernel_init)(net)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = Flatten()(net)\n",
    "\n",
    "    net = Dense(units=128, use_bias=use_bias)(net)\n",
    "    out = Dense(units=n_classes, activation='softmax', use_bias=use_bias)(net)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=x_in, outputs=out, name='Classifier')\n",
    "\n",
    "    if print_summary:\n",
    "        print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    z = tf.random.uniform(minval=-1, maxval=1, shape=(BATCH_SIZE, z_dim))\n",
    "    # 추가\n",
    "#     u = tf.ones([BATCH_SIZE,10])/10    \n",
    "    # 추가\n",
    "    with tf.GradientTape() as C_tape, tf.GradientTape() as G_tape:\n",
    "        Gz = generator(z, training=True)\n",
    "\n",
    "        C_real = classifier(x, training=True)\n",
    "        C_fake = classifier(Gz, training=True)\n",
    "\n",
    "        C_real_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, C_real))\n",
    "        # 추가\n",
    "#         kl = tf.keras.losses.KLDivergence()\n",
    "#         C_fake_loss = kl(u, C_fake) * (-1) # uniform 안되도록 kl(u, C_fake)는 uniform 될수록 작아져\n",
    "#         C_loss = C_real_loss + C_fake_loss\n",
    "\n",
    "        C_fake_loss = tf.reduce_mean((-1) * C_fake * tf.math.log(C_fake+1e-6))\n",
    "        C_loss = C_real_loss - C_fake_loss\n",
    "\n",
    "    C_grad = C_tape.gradient(C_loss, classifier.trainable_variables)\n",
    "    G_grad = G_tape.gradient(C_fake_loss, generator.trainable_variables)\n",
    "\n",
    "    C_opt.apply_gradients(zip(C_grad, classifier.trainable_variables))\n",
    "    G_opt.apply_gradients(zip(G_grad, generator.trainable_variables))\n",
    "\n",
    "    return C_real_loss, C_fake_loss\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    pbar = tqdm(range(epochs))\n",
    "    losses = []\n",
    "    for epoch in pbar:\n",
    "        for batch in dataset:\n",
    "            #print(batch[0].shape, batch[1].shape)\n",
    "            _loss = train_step(x=batch[0], y=batch[1])\n",
    "            losses.append(_loss)\n",
    "        pbar.set_description(\n",
    "                '[Joint Training of Classifier & Generator] | C-CrossEnt: {:.4f} | G-Ent: {:.4f}'.format(\n",
    "                        *np.array(losses).mean(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 1024\n",
    "z_dim = 64\n",
    "\n",
    "(train_images, train_classes), (test_images, test_classes) = load_data()\n",
    "train_images, test_images = train_images.astype(np.float32)/255, test_images.astype(np.float32)/255\n",
    "train_images, test_images = train_images*2 - 1, test_images*2 - 1\n",
    "train_images, test_images = np.expand_dims(train_images, 3), np.expand_dims(test_images, 3)\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "train_labels = enc.fit_transform(train_classes.reshape(-1,1))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE, 0, True).batch(BATCH_SIZE)\n",
    "\n",
    "generator = define_G(z_dim, True, False)\n",
    "classifier = define_C((28, 28, 1), 10, True, False)\n",
    "\n",
    "C_opt = tf.keras.optimizers.Adam(learning_rate=1E-4, beta_1=0.5)\n",
    "G_opt = tf.keras.optimizers.Adam(learning_rate=1E-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0bbb3bcb73e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-92c4db48106c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;31m#print(batch[0].shape, batch[1].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0m_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         pbar.set_description(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_dataset, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Joint Training of Classifier & Generator] | C-CrossEnt: 0.1144 | G-Ent: 0.0383: 100%|███| 2/2 [00:45<00:00, 22.70s/it]\n"
     ]
    }
   ],
   "source": [
    "train(train_dataset, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save_weights(\"generator_kl_epoch_{}.h5\".format(100))\n",
    "classifier.save_weights(\"classifier_kl_epoch_{}.h5\".format(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(c_train_images, c_train_classes), (c_test_images, c_test_classes) = tf.keras.datasets.cifar10.load_data()\n",
    "c_train_images, c_test_images = c_train_images.astype(np.float32)/255, c_test_images.astype(np.float32)/255\n",
    "c_train_images, c_test_images = c_train_images*2 - 1, c_test_images*2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_cifar10 = []\n",
    "for img in c_test_images:\n",
    "    gray_img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "    resized_cifar10.append(cv2.resize(gray_img, (28, 28), interpolation=cv2.INTER_CUBIC))\n",
    "cifar10_images = np.array(resized_cifar10)\n",
    "cifar10_images = np.expand_dims(cifar10_images, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.load_weights(\"generator_kl_epoch_100.h5\")\n",
    "# classifier.load_weights(\"classifier_kl_epoch_100.h5\")\n",
    "generator.load_weights(\"generator_epoch_500.h5\")\n",
    "classifier.load_weights(\"classifier_epoch_500.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.uniform(-1,1,(10000, 64))\n",
    "generated_images = generator.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = 10, 10\n",
    "figure, axes = plt.subplots(rows, columns)\n",
    "for row in range(rows):\n",
    "    for col in range(columns):\n",
    "        image = generated_images[row + rows * col, :, :, :]\n",
    "        axes[row, col].imshow(np.squeeze(image), cmap='gray')\n",
    "        axes[row, col].axis('off')\n",
    "figure.savefig('C:/GAN/venv/images/{}.png'.format('test1'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result1 = classifier.predict(generated_images)\n",
    "test_result2 = classifier.predict(cifar10_images)\n",
    "test_result3 = classifier.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels = np.argmax(test_result3,1)\n",
    "accuracy_score(test_classes,predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.ones(10000)\n",
    "len(arr[np.max(test_result2,1) < 0.99999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_Conditional_G(z_dim, n_classes=10, use_bias=False, print_summary=True):\n",
    "    kernel_init = 'glorot_normal'\n",
    "    \n",
    "    in_label = Input(shape=(1,))\n",
    "\t# embedding for categorical input\n",
    "\tli = Embedding(n_classes, 50)(in_label)\n",
    "\t# linear multiplication\n",
    "\tn_nodes = 7 * 7\n",
    "\tli = Dense(n_nodes)(li)\n",
    "\t# reshape to additional channel\n",
    "\tli = Reshape((7, 7, 1))(li)\n",
    "    z_in = Input(shape=(z_dim,))\n",
    "\n",
    "    net = Dense(units=128, kernel_regularizer=l2(1E-5), use_bias=use_bias)(z_in)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = LeakyReLU(alpha=0.15)(net)\n",
    "   \n",
    "    net = Dense(units=7*7*32, kernel_regularizer=l2(1E-5), use_bias=use_bias)(net)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = LeakyReLU(alpha=0.15)(net)\n",
    "    net = Reshape((7, 7, 32))(net)\n",
    "    \n",
    "    merge = Concatenate()([net,li])\n",
    "    \n",
    "    net = Conv2DTranspose(filters=16, kernel_size=(5, 5), strides=(2, 2), use_bias=use_bias,\n",
    "                          kernel_regularizer=l2(1E-5), padding='same', kernel_initializer=kernel_init)(merge)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = LeakyReLU(alpha=0.15)(net)\n",
    "\n",
    "    out = Conv2DTranspose(filters=1, kernel_size=(5, 5), strides=(2, 2), use_bias=use_bias,\n",
    "                          padding='same', kernel_initializer=kernel_init, activation='tanh')(net)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[z_in,in_label], outputs=out, name='Generator')\n",
    "\n",
    "    if print_summary:\n",
    "        print(model.summary())\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
