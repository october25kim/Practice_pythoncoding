{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 구현에 필요한 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용할 게임 환경 설정 & Hyper parameter 정의하기\n",
    "\n",
    "게임 환경 : pendulum-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "H1 = 400  # Number of hidden nodes\n",
    "H2 = 300  # Number of hidden nodes\n",
    "EPS = 0.03\n",
    "EPS_DECAY = 0.001\n",
    "BATCH_SIZE = 128\n",
    "N_EPISODE = 200\n",
    "MAX_STEP = 1000\n",
    "TAU = 0.001  # Soft-update\n",
    "LR = 0.001  # Critic learning rate\n",
    "LR_A = 0.0001  # Actor learning rate\n",
    "GAMMA = 0.99  # Reward discount\n",
    "EPSILON_MIN = 0.001\n",
    "EPS_DECAY = 0.001  # EPSILON decay\n",
    "EPSILON = 1.0  # Greedy policy\n",
    "MEMORY_CAPACITY = 1000000  # Update frequency\n",
    "EPS_DECAY = 0.001  # EPSILON decay\n",
    "USE_CUDA = torch.cuda.is_available()  # Use GPU\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "N_ACTIONS = env.action_space.shape[0]\n",
    "ACTIONS_LIM = env.action_space.high[0]\n",
    "N_STATES = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Buffer 정의 \n",
    "\n",
    "경험(s, a, r, t, s_)를 구성하여 replay buffer에 저장\n",
    "\n",
    "replay buffer의 수용량이 초과할 경우 저장된 경험에서 일부 가져와 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, buffer_size, random_seed=123):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = []\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, s, a, r, t, s2):\n",
    "        experience = (s, a, r, t, s2)\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.pop(0)\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch])\n",
    "        t_batch = np.array([_[3] for _ in batch])\n",
    "        s2_batch = np.array([_[4] for _ in batch])\n",
    "\n",
    "        return s_batch, a_batch, r_batch, t_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer = []\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력(Input)인 게임 상태정보에 대해 Noise를 추가하는 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"docstring for OUNoise\"\"\"\n",
    "    def __init__(self, action_dimension, mu=0, theta=0.15, sigma=0.2):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Neural Network 구현\n",
    "\n",
    "모든 노드 간에 연결 되어있는 모델\n",
    "\n",
    "입력(Input) : 게임의 상태 정보\n",
    "\n",
    "출력(Output) : Critic network의 경우 행동 가치 함수의 반환 값 / Actor network의 경우 정책 함수의 반환 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create actor-critic network\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(N_STATES, H1)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)\n",
    "    \n",
    "        self.fc2 = nn.Linear(H1 + N_ACTIONS, H2)\n",
    "        self.fc2.weight.data.normal_(0, 0.1)\n",
    "    \n",
    "        self.fc3 = nn.Linear(H2, 1)\n",
    "        self.fc3.weight.data.uniform_(-EPS, EPS)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        return critic Q(s,a)\n",
    "        :param state: state [n, state_dim] (n is batch_size)\n",
    "        :param action: action [n, action_dim]\n",
    "        :return: Q(s,a) [n, 1]\n",
    "        \"\"\"\n",
    "        \n",
    "        s1 = self.relu(self.fc1(state))\n",
    "        x = torch.cat((s1, action), dim=1)\n",
    "        \n",
    "        x = self.relu(self.fc2(x))\n",
    "        action_value = self.fc3(x)\n",
    "        \n",
    "        return action_value\n",
    "    \n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        :param state_dim: int\n",
    "        :param action_dim: int\n",
    "        :param action_lim: Used to limit action space in [-action_lim,action_lim]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(N_STATES, H1)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)\n",
    "    \n",
    "        self.fc2 = nn.Linear(H1, H2)\n",
    "        self.fc2.weight.data.normal_(0, 0.1)\n",
    "        \n",
    "        self.fc3 = nn.Linear(H2, N_ACTIONS)\n",
    "        self.fc3.weight.data.uniform_(-EPS, EPS)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        return actor policy function Pi(s)\n",
    "        :param state: state [n, state_dim]\n",
    "        :return: action [n, action_dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.relu(self.fc1(state))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        action = self.tanh(self.fc3(x))  # tanh limit (-1, 1)\n",
    "        return action\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG Agent 정의\n",
    "\n",
    "1. Actor Network와 Critic Network를 정의 & Optimizer(최적화 기법)와 손실 함수(Loss function)정의\n",
    "\n",
    "2. 행동 선택 : 연속적인 행동 (-2,2)에서 상태 정보에 노이즈를 추가하여 (-1,1) 범위의 행동으로 수정 및 선택\n",
    "\n",
    "continuous action spaces에서의 주된 과제는 탐험(exploration)이다. DDPG는 지속적인 탐험을 위해 action에 노이즈를 준다.\n",
    "\n",
    "3. 경험(s, a, r, t, s_)이 저장된 replay buffer로부터 batch size만큼 가져와 tensor형태로 변환 후, 손실 함수(Loss function)를 최소화하는 방향으로 가중치를 학습\n",
    "\n",
    "4. Soft target update는 업데이트 되는 Q train network가 target network 값의 계산에도 사용되기 때문에, Q 업데이트가 발산하는 경향이 있다.\n",
    "\n",
    "따라서 target network 값을 계산하는데 사용되는 actor와 critic train network의 weights를 복사한다.\n",
    "\n",
    "이 target network의 weights는 학습된 networks를 천천히 추적하도록 함으로써 업데이트 된다.\n",
    "\n",
    "즉 target network 값을 천천히 변하도록 제한하여 학습의 안정성을 높이게 되는 것이다.\n",
    "\n",
    "5. Critic network의 손실 함수를 최소화하는 방향으로 가중치 업데이트 후, Actor network의 가중치 학습을 진행. 이때 Critic network에서 업데이트한 가중치를 포함하여 정책 함수의 가중치를 업데이트 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(object):\n",
    "    def __init__(self):\n",
    "        self.is_training = True\n",
    "        self.epsilon = EPSILON\n",
    "        self.eps_decay = EPS_DECAY\n",
    "        self.randomer = OUNoise(N_ACTIONS)\n",
    "        self.buffer = ReplayBuffer(MEMORY_CAPACITY)\n",
    "        \n",
    "        self.actor = Actor()  # Train network\n",
    "        self.actor_target = Actor()  # Target network\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=LR_A)\n",
    "        \n",
    "        self.critic = Critic()  # Train network\n",
    "        self.critic_target = Critic()  # Target network\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=LR)\n",
    "        \n",
    "        self.hard_update(self.actor_target, self.actor)\n",
    "        self.hard_update(self.critic_target, self.critic)\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            self.cuda()\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            state = state.cuda()\n",
    "            \n",
    "        action = self.actor(state).detach()\n",
    "        action = action.squeeze(0).cpu().numpy()\n",
    "        action += self.is_training * max(EPSILON, EPSILON_MIN) * self.randomer.noise()\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        \n",
    "        self.action = action\n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        s1, a1, r1, t1, s2 = self.buffer.sample_batch(BATCH_SIZE)\n",
    "        # bool -> int\n",
    "        t1 = (t1 == False) * 1\n",
    "        s1 = torch.tensor(s1, dtype=torch.float)\n",
    "        a1 = torch.tensor(a1, dtype=torch.float)\n",
    "        r1 = torch.tensor(r1, dtype=torch.float)\n",
    "        t1 = torch.tensor(t1, dtype=torch.float)\n",
    "        s2 = torch.tensor(s2, dtype=torch.float)\n",
    "        if USE_CUDA:\n",
    "            s1 = s1.cuda()\n",
    "            a1 = a1.cuda()\n",
    "            r1 = r1.cuda()\n",
    "            t1 = t1.cuda()\n",
    "            s2 = s2.cuda()\n",
    "        \n",
    "        a2 = self.actor_target(s2).detach()  # Don't backporpagate\n",
    "        target_q = self.critic_target(s2, a2).detach()  # Don't backporpagate\n",
    "        y_expected = r1[:, None] + t1[:, None] * GAMMA * target_q\n",
    "        y_predicted = self.critic.forward(s1, a1)\n",
    "        \n",
    "        # critic gradient\n",
    "        critic_loss = nn.MSELoss()\n",
    "        loss_critic = critic_loss(y_predicted, y_expected)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # actor gradient\n",
    "        pred_a = self.actor.forward(s1)\n",
    "        loss_actor = (-self.critic.forward(s1, pred_a)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.actor_target, self.actor, TAU)\n",
    "        self.soft_update(self.critic_target, self.critic, TAU)\n",
    "        \n",
    "        return loss_actor.item(), loss_critic.item()\n",
    "    \n",
    "    def cuda(self):\n",
    "        self.actor.cuda()\n",
    "        self.actor_target.cuda()\n",
    "        self.critic.cuda()\n",
    "        self.critic_target.cuda()\n",
    "        \n",
    "    def soft_update(self, target, source, tau=0.001):\n",
    "        \"\"\"\n",
    "        update target by target = tau * source + (1 - tau) * target\n",
    "        :param target: Target network\n",
    "        :param source: source network\n",
    "        :param tau: 0 < tau << 1\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - tau) + param.data * tau\n",
    "            )\n",
    "\n",
    "    def hard_update(self, target, source):\n",
    "        \"\"\"\n",
    "        update target by target = source\n",
    "        :param target: Target network\n",
    "        :param source: source network\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "                target_param.data.copy_(param.data)  \n",
    "     \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon -= self.eps_decay\n",
    "\n",
    "    def reset(self):\n",
    "        self.randomer.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG agent define\n",
    "agent = DDPG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습\n",
    "\n",
    "에피소드 내에 제한된 Timestep에서 경험을 생성하여 저장 후 BATCH_SIZE보다 클 경우 학습을 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(pre_episodes=0, pre_total_step=0):\n",
    "    total_step = pre_total_step\n",
    "    \n",
    "    all_rewards = []\n",
    "    for ep in range(pre_episodes + 1, N_EPISODE + 1):\n",
    "        s0 = env.reset()\n",
    "        agent.reset()\n",
    "        \n",
    "        done = False\n",
    "        step = 0\n",
    "        actor_loss, critics_loss, reward = 0, 0, 0\n",
    "        \n",
    "        # decay noise\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        while not done:\n",
    "            #env.render()\n",
    "            action = agent.select_action(s0)\n",
    "            \n",
    "            s1, r1, done, info = env.step(action)\n",
    "            agent.buffer.add(s0, action, r1, done, s1)\n",
    "            s0 = s1\n",
    "            \n",
    "            if agent.buffer.size() > BATCH_SIZE:\n",
    "                loss_a, loss_c = agent.learn()\n",
    "                actor_loss += loss_a\n",
    "                critics_loss += loss_c\n",
    "                \n",
    "            reward += r1\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "            \n",
    "            if step + 1 > MAX_STEP:\n",
    "                break\n",
    "        \n",
    "        all_rewards.append(reward)\n",
    "        avg_reward = float(np.mean(all_rewards[-100:]))\n",
    "        \n",
    "        print('total step: %5d, episodes %3d, episode_step: %5d, episode_reward: %5f'\n",
    "              % (total_step, ep, step, reward))     \n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    plt.title(\"Pendulum scores for 200 episodes\")\n",
    "    plt.plot(np.arange(200), all_rewards)\n",
    "    plt.xlabel(\"episode\")\n",
    "    plt.ylabel(\"ep_reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
